{
  "best_metric": 0.5587266215383005,
  "best_model_checkpoint": "model\\checkpoint-4797",
  "epoch": 16.0,
  "eval_steps": 500,
  "global_step": 5904,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 15.638930320739746,
      "learning_rate": 1.9994579945799458e-05,
      "loss": 3.5328,
      "step": 10
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 17.429506301879883,
      "learning_rate": 1.9989159891598918e-05,
      "loss": 3.4922,
      "step": 20
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 13.718768119812012,
      "learning_rate": 1.9983739837398374e-05,
      "loss": 3.4555,
      "step": 30
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 19.36943817138672,
      "learning_rate": 1.9978319783197834e-05,
      "loss": 3.4495,
      "step": 40
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 11.800071716308594,
      "learning_rate": 1.9972899728997294e-05,
      "loss": 3.4733,
      "step": 50
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 13.123390197753906,
      "learning_rate": 1.996747967479675e-05,
      "loss": 3.4108,
      "step": 60
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 16.632312774658203,
      "learning_rate": 1.9962059620596206e-05,
      "loss": 3.4542,
      "step": 70
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 14.272778511047363,
      "learning_rate": 1.9956639566395666e-05,
      "loss": 3.414,
      "step": 80
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 15.903769493103027,
      "learning_rate": 1.9951219512195123e-05,
      "loss": 3.4036,
      "step": 90
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 12.936623573303223,
      "learning_rate": 1.9945799457994582e-05,
      "loss": 3.4236,
      "step": 100
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 10.124852180480957,
      "learning_rate": 1.994037940379404e-05,
      "loss": 3.4398,
      "step": 110
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 12.695439338684082,
      "learning_rate": 1.9934959349593495e-05,
      "loss": 3.4319,
      "step": 120
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 11.052013397216797,
      "learning_rate": 1.9929539295392955e-05,
      "loss": 3.4182,
      "step": 130
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 10.603812217712402,
      "learning_rate": 1.9924119241192415e-05,
      "loss": 3.3958,
      "step": 140
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 12.25672435760498,
      "learning_rate": 1.991869918699187e-05,
      "loss": 3.4136,
      "step": 150
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 11.148934364318848,
      "learning_rate": 1.991327913279133e-05,
      "loss": 3.3935,
      "step": 160
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 14.748757362365723,
      "learning_rate": 1.9907859078590787e-05,
      "loss": 3.4196,
      "step": 170
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 12.611199378967285,
      "learning_rate": 1.9902439024390247e-05,
      "loss": 3.4304,
      "step": 180
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 13.480661392211914,
      "learning_rate": 1.9897018970189703e-05,
      "loss": 3.3274,
      "step": 190
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 10.867070198059082,
      "learning_rate": 1.989159891598916e-05,
      "loss": 3.428,
      "step": 200
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 11.497454643249512,
      "learning_rate": 1.988617886178862e-05,
      "loss": 3.3832,
      "step": 210
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 11.304462432861328,
      "learning_rate": 1.9880758807588076e-05,
      "loss": 3.3487,
      "step": 220
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 12.010199546813965,
      "learning_rate": 1.9875338753387536e-05,
      "loss": 3.3855,
      "step": 230
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 15.270101547241211,
      "learning_rate": 1.9869918699186996e-05,
      "loss": 3.3984,
      "step": 240
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 13.475481986999512,
      "learning_rate": 1.9864498644986452e-05,
      "loss": 3.3023,
      "step": 250
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 13.005599021911621,
      "learning_rate": 1.9859078590785908e-05,
      "loss": 3.2523,
      "step": 260
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 10.896656036376953,
      "learning_rate": 1.9853658536585368e-05,
      "loss": 3.2076,
      "step": 270
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 25.33131980895996,
      "learning_rate": 1.9848238482384824e-05,
      "loss": 3.2278,
      "step": 280
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 14.508033752441406,
      "learning_rate": 1.9842818428184284e-05,
      "loss": 3.1296,
      "step": 290
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 11.233055114746094,
      "learning_rate": 1.983739837398374e-05,
      "loss": 3.1609,
      "step": 300
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 14.214853286743164,
      "learning_rate": 1.98319783197832e-05,
      "loss": 3.0833,
      "step": 310
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 17.250703811645508,
      "learning_rate": 1.9826558265582657e-05,
      "loss": 3.0833,
      "step": 320
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 10.725478172302246,
      "learning_rate": 1.9821138211382117e-05,
      "loss": 3.197,
      "step": 330
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 10.567273139953613,
      "learning_rate": 1.9815718157181573e-05,
      "loss": 3.1085,
      "step": 340
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 12.181428909301758,
      "learning_rate": 1.9810298102981033e-05,
      "loss": 3.1519,
      "step": 350
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 12.333513259887695,
      "learning_rate": 1.980487804878049e-05,
      "loss": 2.9801,
      "step": 360
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.22357723577235772,
      "eval_f1": 0.21230462764117955,
      "eval_loss": 2.911597967147827,
      "eval_runtime": 2.6624,
      "eval_samples_per_second": 277.197,
      "eval_steps_per_second": 34.931,
      "step": 369
    },
    {
      "epoch": 1.002710027100271,
      "grad_norm": 11.400879859924316,
      "learning_rate": 1.9799457994579945e-05,
      "loss": 2.8505,
      "step": 370
    },
    {
      "epoch": 1.029810298102981,
      "grad_norm": 12.757988929748535,
      "learning_rate": 1.9794037940379405e-05,
      "loss": 2.9895,
      "step": 380
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 9.554718971252441,
      "learning_rate": 1.978861788617886e-05,
      "loss": 2.9129,
      "step": 390
    },
    {
      "epoch": 1.084010840108401,
      "grad_norm": 10.492857933044434,
      "learning_rate": 1.978319783197832e-05,
      "loss": 2.8651,
      "step": 400
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 12.430553436279297,
      "learning_rate": 1.977777777777778e-05,
      "loss": 2.9249,
      "step": 410
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 11.849715232849121,
      "learning_rate": 1.9772357723577238e-05,
      "loss": 3.0038,
      "step": 420
    },
    {
      "epoch": 1.165311653116531,
      "grad_norm": 11.564635276794434,
      "learning_rate": 1.9766937669376697e-05,
      "loss": 2.8202,
      "step": 430
    },
    {
      "epoch": 1.1924119241192412,
      "grad_norm": 10.911909103393555,
      "learning_rate": 1.9761517615176154e-05,
      "loss": 2.812,
      "step": 440
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 9.969083786010742,
      "learning_rate": 1.975609756097561e-05,
      "loss": 2.7763,
      "step": 450
    },
    {
      "epoch": 1.2466124661246614,
      "grad_norm": 10.78076171875,
      "learning_rate": 1.975067750677507e-05,
      "loss": 2.8263,
      "step": 460
    },
    {
      "epoch": 1.2737127371273713,
      "grad_norm": 11.564924240112305,
      "learning_rate": 1.9745257452574526e-05,
      "loss": 2.859,
      "step": 470
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 11.067591667175293,
      "learning_rate": 1.9739837398373986e-05,
      "loss": 2.9669,
      "step": 480
    },
    {
      "epoch": 1.3279132791327912,
      "grad_norm": 9.726137161254883,
      "learning_rate": 1.9734417344173446e-05,
      "loss": 2.7637,
      "step": 490
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 10.306234359741211,
      "learning_rate": 1.9728997289972902e-05,
      "loss": 2.5929,
      "step": 500
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 10.136109352111816,
      "learning_rate": 1.972357723577236e-05,
      "loss": 2.5075,
      "step": 510
    },
    {
      "epoch": 1.4092140921409215,
      "grad_norm": 9.87122631072998,
      "learning_rate": 1.971815718157182e-05,
      "loss": 2.6194,
      "step": 520
    },
    {
      "epoch": 1.4363143631436315,
      "grad_norm": 9.862357139587402,
      "learning_rate": 1.9712737127371275e-05,
      "loss": 2.5538,
      "step": 530
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 8.130999565124512,
      "learning_rate": 1.9707317073170734e-05,
      "loss": 2.7863,
      "step": 540
    },
    {
      "epoch": 1.4905149051490514,
      "grad_norm": 9.799887657165527,
      "learning_rate": 1.970189701897019e-05,
      "loss": 2.6447,
      "step": 550
    },
    {
      "epoch": 1.5176151761517616,
      "grad_norm": 8.597997665405273,
      "learning_rate": 1.9696476964769647e-05,
      "loss": 2.6933,
      "step": 560
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 9.139711380004883,
      "learning_rate": 1.9691056910569107e-05,
      "loss": 2.6801,
      "step": 570
    },
    {
      "epoch": 1.5718157181571817,
      "grad_norm": 9.152627944946289,
      "learning_rate": 1.9685636856368567e-05,
      "loss": 2.8463,
      "step": 580
    },
    {
      "epoch": 1.5989159891598916,
      "grad_norm": 10.656366348266602,
      "learning_rate": 1.9680216802168023e-05,
      "loss": 2.9546,
      "step": 590
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 10.403901100158691,
      "learning_rate": 1.9674796747967483e-05,
      "loss": 2.5285,
      "step": 600
    },
    {
      "epoch": 1.6531165311653115,
      "grad_norm": 9.538745880126953,
      "learning_rate": 1.966937669376694e-05,
      "loss": 2.8352,
      "step": 610
    },
    {
      "epoch": 1.6802168021680217,
      "grad_norm": 36.55433654785156,
      "learning_rate": 1.9663956639566396e-05,
      "loss": 2.6894,
      "step": 620
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 9.106127738952637,
      "learning_rate": 1.9658536585365856e-05,
      "loss": 2.7448,
      "step": 630
    },
    {
      "epoch": 1.7344173441734418,
      "grad_norm": 10.799108505249023,
      "learning_rate": 1.9653116531165312e-05,
      "loss": 2.4716,
      "step": 640
    },
    {
      "epoch": 1.7615176151761518,
      "grad_norm": 11.325089454650879,
      "learning_rate": 1.964769647696477e-05,
      "loss": 2.7525,
      "step": 650
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 9.358609199523926,
      "learning_rate": 1.9642276422764228e-05,
      "loss": 2.7134,
      "step": 660
    },
    {
      "epoch": 1.8157181571815717,
      "grad_norm": 11.98109245300293,
      "learning_rate": 1.9636856368563688e-05,
      "loss": 2.6677,
      "step": 670
    },
    {
      "epoch": 1.8428184281842819,
      "grad_norm": 10.430764198303223,
      "learning_rate": 1.9631436314363144e-05,
      "loss": 2.6573,
      "step": 680
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 9.35030460357666,
      "learning_rate": 1.9626016260162604e-05,
      "loss": 2.4919,
      "step": 690
    },
    {
      "epoch": 1.897018970189702,
      "grad_norm": 17.85734748840332,
      "learning_rate": 1.962059620596206e-05,
      "loss": 2.6637,
      "step": 700
    },
    {
      "epoch": 1.924119241192412,
      "grad_norm": 12.782122611999512,
      "learning_rate": 1.961517615176152e-05,
      "loss": 2.5549,
      "step": 710
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 9.919051170349121,
      "learning_rate": 1.9609756097560977e-05,
      "loss": 2.5925,
      "step": 720
    },
    {
      "epoch": 1.9783197831978319,
      "grad_norm": 11.709147453308105,
      "learning_rate": 1.9604336043360436e-05,
      "loss": 2.5675,
      "step": 730
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.3075880758807588,
      "eval_f1": 0.3121820222541215,
      "eval_loss": 2.4985926151275635,
      "eval_runtime": 2.7612,
      "eval_samples_per_second": 267.275,
      "eval_steps_per_second": 33.681,
      "step": 738
    },
    {
      "epoch": 2.005420054200542,
      "grad_norm": 9.967063903808594,
      "learning_rate": 1.9598915989159893e-05,
      "loss": 2.615,
      "step": 740
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 14.603716850280762,
      "learning_rate": 1.959349593495935e-05,
      "loss": 2.5399,
      "step": 750
    },
    {
      "epoch": 2.059620596205962,
      "grad_norm": 9.519803047180176,
      "learning_rate": 1.958807588075881e-05,
      "loss": 2.5004,
      "step": 760
    },
    {
      "epoch": 2.086720867208672,
      "grad_norm": 10.966314315795898,
      "learning_rate": 1.958265582655827e-05,
      "loss": 2.3853,
      "step": 770
    },
    {
      "epoch": 2.113821138211382,
      "grad_norm": 11.09739875793457,
      "learning_rate": 1.9577235772357725e-05,
      "loss": 2.4219,
      "step": 780
    },
    {
      "epoch": 2.140921409214092,
      "grad_norm": 11.853072166442871,
      "learning_rate": 1.9571815718157185e-05,
      "loss": 2.4644,
      "step": 790
    },
    {
      "epoch": 2.168021680216802,
      "grad_norm": 9.34618854522705,
      "learning_rate": 1.956639566395664e-05,
      "loss": 2.4429,
      "step": 800
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 11.61600399017334,
      "learning_rate": 1.9560975609756098e-05,
      "loss": 2.257,
      "step": 810
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 9.842159271240234,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 2.6068,
      "step": 820
    },
    {
      "epoch": 2.2493224932249323,
      "grad_norm": 11.82221508026123,
      "learning_rate": 1.9550135501355014e-05,
      "loss": 2.2727,
      "step": 830
    },
    {
      "epoch": 2.2764227642276422,
      "grad_norm": 9.526747703552246,
      "learning_rate": 1.9544715447154473e-05,
      "loss": 2.466,
      "step": 840
    },
    {
      "epoch": 2.303523035230352,
      "grad_norm": 9.76457691192627,
      "learning_rate": 1.9539295392953933e-05,
      "loss": 2.6628,
      "step": 850
    },
    {
      "epoch": 2.330623306233062,
      "grad_norm": 8.829988479614258,
      "learning_rate": 1.953387533875339e-05,
      "loss": 2.466,
      "step": 860
    },
    {
      "epoch": 2.3577235772357725,
      "grad_norm": 14.41539478302002,
      "learning_rate": 1.9528455284552846e-05,
      "loss": 2.5912,
      "step": 870
    },
    {
      "epoch": 2.3848238482384825,
      "grad_norm": 8.872146606445312,
      "learning_rate": 1.9523035230352306e-05,
      "loss": 2.5417,
      "step": 880
    },
    {
      "epoch": 2.4119241192411924,
      "grad_norm": 9.9342622756958,
      "learning_rate": 1.9517615176151762e-05,
      "loss": 2.5137,
      "step": 890
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 12.209735870361328,
      "learning_rate": 1.9512195121951222e-05,
      "loss": 2.3607,
      "step": 900
    },
    {
      "epoch": 2.4661246612466123,
      "grad_norm": 9.693079948425293,
      "learning_rate": 1.950677506775068e-05,
      "loss": 2.2777,
      "step": 910
    },
    {
      "epoch": 2.4932249322493227,
      "grad_norm": 10.175986289978027,
      "learning_rate": 1.9501355013550135e-05,
      "loss": 2.3449,
      "step": 920
    },
    {
      "epoch": 2.5203252032520327,
      "grad_norm": 9.811588287353516,
      "learning_rate": 1.9495934959349594e-05,
      "loss": 2.5148,
      "step": 930
    },
    {
      "epoch": 2.5474254742547426,
      "grad_norm": 12.345367431640625,
      "learning_rate": 1.9490514905149054e-05,
      "loss": 2.5095,
      "step": 940
    },
    {
      "epoch": 2.5745257452574526,
      "grad_norm": 11.852256774902344,
      "learning_rate": 1.948509485094851e-05,
      "loss": 2.6322,
      "step": 950
    },
    {
      "epoch": 2.6016260162601625,
      "grad_norm": 9.434151649475098,
      "learning_rate": 1.947967479674797e-05,
      "loss": 2.5564,
      "step": 960
    },
    {
      "epoch": 2.6287262872628725,
      "grad_norm": 14.54719352722168,
      "learning_rate": 1.9474254742547427e-05,
      "loss": 2.4667,
      "step": 970
    },
    {
      "epoch": 2.6558265582655824,
      "grad_norm": 13.87148380279541,
      "learning_rate": 1.9468834688346883e-05,
      "loss": 2.669,
      "step": 980
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 13.003678321838379,
      "learning_rate": 1.9463414634146343e-05,
      "loss": 2.502,
      "step": 990
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 11.05121898651123,
      "learning_rate": 1.94579945799458e-05,
      "loss": 2.492,
      "step": 1000
    },
    {
      "epoch": 2.7371273712737128,
      "grad_norm": 10.514497756958008,
      "learning_rate": 1.945257452574526e-05,
      "loss": 2.2393,
      "step": 1010
    },
    {
      "epoch": 2.7642276422764227,
      "grad_norm": 9.454615592956543,
      "learning_rate": 1.944715447154472e-05,
      "loss": 2.4267,
      "step": 1020
    },
    {
      "epoch": 2.7913279132791327,
      "grad_norm": 8.842045783996582,
      "learning_rate": 1.9441734417344175e-05,
      "loss": 2.2381,
      "step": 1030
    },
    {
      "epoch": 2.818428184281843,
      "grad_norm": 11.361824989318848,
      "learning_rate": 1.9436314363143635e-05,
      "loss": 2.3257,
      "step": 1040
    },
    {
      "epoch": 2.845528455284553,
      "grad_norm": 11.905810356140137,
      "learning_rate": 1.943089430894309e-05,
      "loss": 2.5193,
      "step": 1050
    },
    {
      "epoch": 2.872628726287263,
      "grad_norm": 9.576217651367188,
      "learning_rate": 1.9425474254742548e-05,
      "loss": 2.25,
      "step": 1060
    },
    {
      "epoch": 2.899728997289973,
      "grad_norm": 10.641104698181152,
      "learning_rate": 1.9420054200542008e-05,
      "loss": 2.3343,
      "step": 1070
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 12.747092247009277,
      "learning_rate": 1.9414634146341464e-05,
      "loss": 2.4082,
      "step": 1080
    },
    {
      "epoch": 2.953929539295393,
      "grad_norm": 9.542268753051758,
      "learning_rate": 1.9409214092140924e-05,
      "loss": 2.4704,
      "step": 1090
    },
    {
      "epoch": 2.9810298102981028,
      "grad_norm": 8.827851295471191,
      "learning_rate": 1.940379403794038e-05,
      "loss": 2.3034,
      "step": 1100
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.34552845528455284,
      "eval_f1": 0.3546288224054718,
      "eval_loss": 2.3521153926849365,
      "eval_runtime": 2.6755,
      "eval_samples_per_second": 275.84,
      "eval_steps_per_second": 34.76,
      "step": 1107
    },
    {
      "epoch": 3.008130081300813,
      "grad_norm": 10.507917404174805,
      "learning_rate": 1.939837398373984e-05,
      "loss": 2.2462,
      "step": 1110
    },
    {
      "epoch": 3.035230352303523,
      "grad_norm": 7.731808662414551,
      "learning_rate": 1.9392953929539296e-05,
      "loss": 2.173,
      "step": 1120
    },
    {
      "epoch": 3.062330623306233,
      "grad_norm": 10.908681869506836,
      "learning_rate": 1.9387533875338756e-05,
      "loss": 2.4135,
      "step": 1130
    },
    {
      "epoch": 3.089430894308943,
      "grad_norm": 8.159634590148926,
      "learning_rate": 1.9382113821138212e-05,
      "loss": 2.3657,
      "step": 1140
    },
    {
      "epoch": 3.116531165311653,
      "grad_norm": 12.08342456817627,
      "learning_rate": 1.9376693766937672e-05,
      "loss": 2.2581,
      "step": 1150
    },
    {
      "epoch": 3.1436314363143634,
      "grad_norm": 11.753584861755371,
      "learning_rate": 1.937127371273713e-05,
      "loss": 2.4115,
      "step": 1160
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 12.881315231323242,
      "learning_rate": 1.9365853658536585e-05,
      "loss": 1.9978,
      "step": 1170
    },
    {
      "epoch": 3.1978319783197833,
      "grad_norm": 10.446946144104004,
      "learning_rate": 1.9360433604336045e-05,
      "loss": 2.138,
      "step": 1180
    },
    {
      "epoch": 3.2249322493224932,
      "grad_norm": 8.34240436553955,
      "learning_rate": 1.93550135501355e-05,
      "loss": 2.269,
      "step": 1190
    },
    {
      "epoch": 3.252032520325203,
      "grad_norm": 13.25066089630127,
      "learning_rate": 1.934959349593496e-05,
      "loss": 2.3849,
      "step": 1200
    },
    {
      "epoch": 3.279132791327913,
      "grad_norm": 6.991366863250732,
      "learning_rate": 1.934417344173442e-05,
      "loss": 2.1018,
      "step": 1210
    },
    {
      "epoch": 3.306233062330623,
      "grad_norm": 9.68232536315918,
      "learning_rate": 1.9338753387533877e-05,
      "loss": 2.0136,
      "step": 1220
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 11.162529945373535,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 2.3853,
      "step": 1230
    },
    {
      "epoch": 3.3604336043360434,
      "grad_norm": 10.959643363952637,
      "learning_rate": 1.9327913279132793e-05,
      "loss": 2.0328,
      "step": 1240
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 13.450553894042969,
      "learning_rate": 1.932249322493225e-05,
      "loss": 2.4365,
      "step": 1250
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 9.940088272094727,
      "learning_rate": 1.931707317073171e-05,
      "loss": 2.0989,
      "step": 1260
    },
    {
      "epoch": 3.4417344173441733,
      "grad_norm": 10.906291007995605,
      "learning_rate": 1.9311653116531166e-05,
      "loss": 2.2793,
      "step": 1270
    },
    {
      "epoch": 3.4688346883468837,
      "grad_norm": 6.832801342010498,
      "learning_rate": 1.9306233062330622e-05,
      "loss": 2.0826,
      "step": 1280
    },
    {
      "epoch": 3.4959349593495936,
      "grad_norm": 9.775858879089355,
      "learning_rate": 1.9300813008130085e-05,
      "loss": 2.3716,
      "step": 1290
    },
    {
      "epoch": 3.5230352303523036,
      "grad_norm": 11.043845176696777,
      "learning_rate": 1.929539295392954e-05,
      "loss": 2.3514,
      "step": 1300
    },
    {
      "epoch": 3.5501355013550135,
      "grad_norm": 12.973289489746094,
      "learning_rate": 1.9289972899728998e-05,
      "loss": 2.3111,
      "step": 1310
    },
    {
      "epoch": 3.5772357723577235,
      "grad_norm": 11.590132713317871,
      "learning_rate": 1.9284552845528458e-05,
      "loss": 2.32,
      "step": 1320
    },
    {
      "epoch": 3.6043360433604335,
      "grad_norm": 13.3057861328125,
      "learning_rate": 1.9279132791327914e-05,
      "loss": 2.36,
      "step": 1330
    },
    {
      "epoch": 3.6314363143631434,
      "grad_norm": 12.908400535583496,
      "learning_rate": 1.9273712737127374e-05,
      "loss": 2.5415,
      "step": 1340
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 11.830098152160645,
      "learning_rate": 1.926829268292683e-05,
      "loss": 2.1337,
      "step": 1350
    },
    {
      "epoch": 3.6856368563685638,
      "grad_norm": 12.305720329284668,
      "learning_rate": 1.9262872628726287e-05,
      "loss": 2.186,
      "step": 1360
    },
    {
      "epoch": 3.7127371273712737,
      "grad_norm": 12.97757339477539,
      "learning_rate": 1.9257452574525747e-05,
      "loss": 2.2536,
      "step": 1370
    },
    {
      "epoch": 3.7398373983739837,
      "grad_norm": 9.61217212677002,
      "learning_rate": 1.9252032520325206e-05,
      "loss": 2.1661,
      "step": 1380
    },
    {
      "epoch": 3.7669376693766936,
      "grad_norm": 10.417729377746582,
      "learning_rate": 1.9246612466124663e-05,
      "loss": 2.1552,
      "step": 1390
    },
    {
      "epoch": 3.794037940379404,
      "grad_norm": 12.713624954223633,
      "learning_rate": 1.9241192411924122e-05,
      "loss": 1.912,
      "step": 1400
    },
    {
      "epoch": 3.821138211382114,
      "grad_norm": 12.779433250427246,
      "learning_rate": 1.923577235772358e-05,
      "loss": 2.2562,
      "step": 1410
    },
    {
      "epoch": 3.848238482384824,
      "grad_norm": 10.387663841247559,
      "learning_rate": 1.9230352303523035e-05,
      "loss": 2.275,
      "step": 1420
    },
    {
      "epoch": 3.875338753387534,
      "grad_norm": 13.88863754272461,
      "learning_rate": 1.9224932249322495e-05,
      "loss": 2.0752,
      "step": 1430
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 10.032279968261719,
      "learning_rate": 1.921951219512195e-05,
      "loss": 2.2122,
      "step": 1440
    },
    {
      "epoch": 3.9295392953929538,
      "grad_norm": 13.755016326904297,
      "learning_rate": 1.921409214092141e-05,
      "loss": 2.385,
      "step": 1450
    },
    {
      "epoch": 3.9566395663956637,
      "grad_norm": 11.69217586517334,
      "learning_rate": 1.920867208672087e-05,
      "loss": 2.3177,
      "step": 1460
    },
    {
      "epoch": 3.983739837398374,
      "grad_norm": 11.332658767700195,
      "learning_rate": 1.9203252032520327e-05,
      "loss": 2.2116,
      "step": 1470
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.38346883468834686,
      "eval_f1": 0.39903072268762246,
      "eval_loss": 2.2309505939483643,
      "eval_runtime": 2.6617,
      "eval_samples_per_second": 277.263,
      "eval_steps_per_second": 34.94,
      "step": 1476
    },
    {
      "epoch": 4.010840108401084,
      "grad_norm": 12.713315963745117,
      "learning_rate": 1.9197831978319784e-05,
      "loss": 2.0422,
      "step": 1480
    },
    {
      "epoch": 4.0379403794037945,
      "grad_norm": 12.299960136413574,
      "learning_rate": 1.9192411924119243e-05,
      "loss": 2.0427,
      "step": 1490
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 12.057191848754883,
      "learning_rate": 1.91869918699187e-05,
      "loss": 2.2832,
      "step": 1500
    },
    {
      "epoch": 4.092140921409214,
      "grad_norm": 11.788708686828613,
      "learning_rate": 1.918157181571816e-05,
      "loss": 2.1121,
      "step": 1510
    },
    {
      "epoch": 4.119241192411924,
      "grad_norm": 15.453821182250977,
      "learning_rate": 1.9176151761517616e-05,
      "loss": 2.1239,
      "step": 1520
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 16.384307861328125,
      "learning_rate": 1.9170731707317072e-05,
      "loss": 2.1344,
      "step": 1530
    },
    {
      "epoch": 4.173441734417344,
      "grad_norm": 9.69876766204834,
      "learning_rate": 1.9165311653116532e-05,
      "loss": 1.9861,
      "step": 1540
    },
    {
      "epoch": 4.200542005420054,
      "grad_norm": 10.516797065734863,
      "learning_rate": 1.9159891598915992e-05,
      "loss": 1.7857,
      "step": 1550
    },
    {
      "epoch": 4.227642276422764,
      "grad_norm": 12.006254196166992,
      "learning_rate": 1.915447154471545e-05,
      "loss": 1.861,
      "step": 1560
    },
    {
      "epoch": 4.254742547425474,
      "grad_norm": 10.395161628723145,
      "learning_rate": 1.9149051490514908e-05,
      "loss": 2.1194,
      "step": 1570
    },
    {
      "epoch": 4.281842818428184,
      "grad_norm": 11.354267120361328,
      "learning_rate": 1.9143631436314365e-05,
      "loss": 2.0155,
      "step": 1580
    },
    {
      "epoch": 4.308943089430894,
      "grad_norm": 9.803614616394043,
      "learning_rate": 1.9138211382113824e-05,
      "loss": 2.1696,
      "step": 1590
    },
    {
      "epoch": 4.336043360433604,
      "grad_norm": 12.302027702331543,
      "learning_rate": 1.913279132791328e-05,
      "loss": 2.1221,
      "step": 1600
    },
    {
      "epoch": 4.363143631436314,
      "grad_norm": 10.853721618652344,
      "learning_rate": 1.9127371273712737e-05,
      "loss": 1.9165,
      "step": 1610
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 8.708165168762207,
      "learning_rate": 1.9121951219512197e-05,
      "loss": 1.8123,
      "step": 1620
    },
    {
      "epoch": 4.417344173441735,
      "grad_norm": 13.010047912597656,
      "learning_rate": 1.9116531165311653e-05,
      "loss": 2.1553,
      "step": 1630
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 11.708806037902832,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 1.9913,
      "step": 1640
    },
    {
      "epoch": 4.471544715447155,
      "grad_norm": 11.075209617614746,
      "learning_rate": 1.9105691056910573e-05,
      "loss": 2.2577,
      "step": 1650
    },
    {
      "epoch": 4.4986449864498645,
      "grad_norm": 10.922741889953613,
      "learning_rate": 1.910027100271003e-05,
      "loss": 1.8092,
      "step": 1660
    },
    {
      "epoch": 4.5257452574525745,
      "grad_norm": 12.637616157531738,
      "learning_rate": 1.9094850948509486e-05,
      "loss": 2.0654,
      "step": 1670
    },
    {
      "epoch": 4.5528455284552845,
      "grad_norm": 16.669782638549805,
      "learning_rate": 1.9089430894308945e-05,
      "loss": 1.9321,
      "step": 1680
    },
    {
      "epoch": 4.579945799457994,
      "grad_norm": 6.885342597961426,
      "learning_rate": 1.90840108401084e-05,
      "loss": 2.1066,
      "step": 1690
    },
    {
      "epoch": 4.607046070460704,
      "grad_norm": 15.351271629333496,
      "learning_rate": 1.907859078590786e-05,
      "loss": 2.0615,
      "step": 1700
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 16.58792495727539,
      "learning_rate": 1.9073170731707318e-05,
      "loss": 2.1052,
      "step": 1710
    },
    {
      "epoch": 4.661246612466124,
      "grad_norm": 12.92519474029541,
      "learning_rate": 1.9067750677506774e-05,
      "loss": 1.9043,
      "step": 1720
    },
    {
      "epoch": 4.688346883468835,
      "grad_norm": 14.590127944946289,
      "learning_rate": 1.9062330623306234e-05,
      "loss": 1.9408,
      "step": 1730
    },
    {
      "epoch": 4.715447154471545,
      "grad_norm": 8.200684547424316,
      "learning_rate": 1.9056910569105694e-05,
      "loss": 2.0084,
      "step": 1740
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 14.747190475463867,
      "learning_rate": 1.905149051490515e-05,
      "loss": 2.0226,
      "step": 1750
    },
    {
      "epoch": 4.769647696476965,
      "grad_norm": 11.952383041381836,
      "learning_rate": 1.904607046070461e-05,
      "loss": 1.9881,
      "step": 1760
    },
    {
      "epoch": 4.796747967479675,
      "grad_norm": 15.675226211547852,
      "learning_rate": 1.9040650406504066e-05,
      "loss": 1.9625,
      "step": 1770
    },
    {
      "epoch": 4.823848238482385,
      "grad_norm": 21.219709396362305,
      "learning_rate": 1.9035230352303523e-05,
      "loss": 2.258,
      "step": 1780
    },
    {
      "epoch": 4.850948509485095,
      "grad_norm": 16.332979202270508,
      "learning_rate": 1.9029810298102982e-05,
      "loss": 1.9533,
      "step": 1790
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 14.88532543182373,
      "learning_rate": 1.902439024390244e-05,
      "loss": 1.8446,
      "step": 1800
    },
    {
      "epoch": 4.905149051490515,
      "grad_norm": 12.647008895874023,
      "learning_rate": 1.90189701897019e-05,
      "loss": 1.8553,
      "step": 1810
    },
    {
      "epoch": 4.932249322493225,
      "grad_norm": 13.916726112365723,
      "learning_rate": 1.901355013550136e-05,
      "loss": 2.152,
      "step": 1820
    },
    {
      "epoch": 4.959349593495935,
      "grad_norm": 20.610198974609375,
      "learning_rate": 1.9008130081300815e-05,
      "loss": 2.1835,
      "step": 1830
    },
    {
      "epoch": 4.9864498644986455,
      "grad_norm": 12.634651184082031,
      "learning_rate": 1.9002710027100275e-05,
      "loss": 1.999,
      "step": 1840
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.4146341463414634,
      "eval_f1": 0.43568053762430264,
      "eval_loss": 2.110588312149048,
      "eval_runtime": 2.698,
      "eval_samples_per_second": 273.538,
      "eval_steps_per_second": 34.47,
      "step": 1845
    },
    {
      "epoch": 5.013550135501355,
      "grad_norm": 10.114545822143555,
      "learning_rate": 1.899728997289973e-05,
      "loss": 2.0143,
      "step": 1850
    },
    {
      "epoch": 5.040650406504065,
      "grad_norm": 12.209492683410645,
      "learning_rate": 1.8991869918699187e-05,
      "loss": 1.6054,
      "step": 1860
    },
    {
      "epoch": 5.067750677506775,
      "grad_norm": 11.877365112304688,
      "learning_rate": 1.8986449864498647e-05,
      "loss": 2.0608,
      "step": 1870
    },
    {
      "epoch": 5.094850948509485,
      "grad_norm": 49.1046028137207,
      "learning_rate": 1.8981029810298103e-05,
      "loss": 1.9969,
      "step": 1880
    },
    {
      "epoch": 5.121951219512195,
      "grad_norm": 12.417524337768555,
      "learning_rate": 1.8975609756097563e-05,
      "loss": 1.8645,
      "step": 1890
    },
    {
      "epoch": 5.149051490514905,
      "grad_norm": 14.334044456481934,
      "learning_rate": 1.8970189701897023e-05,
      "loss": 1.9756,
      "step": 1900
    },
    {
      "epoch": 5.176151761517615,
      "grad_norm": 17.58696746826172,
      "learning_rate": 1.896476964769648e-05,
      "loss": 1.7738,
      "step": 1910
    },
    {
      "epoch": 5.203252032520325,
      "grad_norm": 12.538400650024414,
      "learning_rate": 1.8959349593495936e-05,
      "loss": 1.7343,
      "step": 1920
    },
    {
      "epoch": 5.230352303523035,
      "grad_norm": 12.609230041503906,
      "learning_rate": 1.8953929539295396e-05,
      "loss": 1.9224,
      "step": 1930
    },
    {
      "epoch": 5.257452574525745,
      "grad_norm": 14.64991569519043,
      "learning_rate": 1.8948509485094852e-05,
      "loss": 1.6703,
      "step": 1940
    },
    {
      "epoch": 5.284552845528455,
      "grad_norm": 17.742094039916992,
      "learning_rate": 1.8943089430894312e-05,
      "loss": 1.9367,
      "step": 1950
    },
    {
      "epoch": 5.311653116531165,
      "grad_norm": 13.747941970825195,
      "learning_rate": 1.8937669376693768e-05,
      "loss": 1.8944,
      "step": 1960
    },
    {
      "epoch": 5.338753387533876,
      "grad_norm": 14.971539497375488,
      "learning_rate": 1.8932249322493224e-05,
      "loss": 1.6649,
      "step": 1970
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 14.339452743530273,
      "learning_rate": 1.8926829268292684e-05,
      "loss": 1.9521,
      "step": 1980
    },
    {
      "epoch": 5.392953929539296,
      "grad_norm": 15.143722534179688,
      "learning_rate": 1.8921409214092144e-05,
      "loss": 1.7598,
      "step": 1990
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 11.949295043945312,
      "learning_rate": 1.89159891598916e-05,
      "loss": 1.9836,
      "step": 2000
    },
    {
      "epoch": 5.4471544715447155,
      "grad_norm": 11.029391288757324,
      "learning_rate": 1.891056910569106e-05,
      "loss": 2.06,
      "step": 2010
    },
    {
      "epoch": 5.4742547425474255,
      "grad_norm": 10.135854721069336,
      "learning_rate": 1.8905149051490517e-05,
      "loss": 1.8639,
      "step": 2020
    },
    {
      "epoch": 5.5013550135501355,
      "grad_norm": 21.37757110595703,
      "learning_rate": 1.8899728997289973e-05,
      "loss": 1.5988,
      "step": 2030
    },
    {
      "epoch": 5.528455284552845,
      "grad_norm": 14.021146774291992,
      "learning_rate": 1.8894308943089433e-05,
      "loss": 2.0557,
      "step": 2040
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 7.343410491943359,
      "learning_rate": 1.888888888888889e-05,
      "loss": 1.515,
      "step": 2050
    },
    {
      "epoch": 5.582655826558265,
      "grad_norm": 10.881501197814941,
      "learning_rate": 1.888346883468835e-05,
      "loss": 1.7413,
      "step": 2060
    },
    {
      "epoch": 5.609756097560975,
      "grad_norm": 11.567828178405762,
      "learning_rate": 1.8878048780487805e-05,
      "loss": 1.7596,
      "step": 2070
    },
    {
      "epoch": 5.636856368563686,
      "grad_norm": 9.222472190856934,
      "learning_rate": 1.8872628726287265e-05,
      "loss": 1.9188,
      "step": 2080
    },
    {
      "epoch": 5.663956639566395,
      "grad_norm": 14.869583129882812,
      "learning_rate": 1.8867208672086725e-05,
      "loss": 2.1377,
      "step": 2090
    },
    {
      "epoch": 5.691056910569106,
      "grad_norm": 15.050888061523438,
      "learning_rate": 1.886178861788618e-05,
      "loss": 1.6768,
      "step": 2100
    },
    {
      "epoch": 5.718157181571816,
      "grad_norm": 16.738113403320312,
      "learning_rate": 1.8856368563685638e-05,
      "loss": 1.5925,
      "step": 2110
    },
    {
      "epoch": 5.745257452574526,
      "grad_norm": 16.62205696105957,
      "learning_rate": 1.8850948509485097e-05,
      "loss": 1.9842,
      "step": 2120
    },
    {
      "epoch": 5.772357723577236,
      "grad_norm": 16.521574020385742,
      "learning_rate": 1.8845528455284554e-05,
      "loss": 1.8713,
      "step": 2130
    },
    {
      "epoch": 5.799457994579946,
      "grad_norm": 13.745966911315918,
      "learning_rate": 1.8840108401084014e-05,
      "loss": 1.8872,
      "step": 2140
    },
    {
      "epoch": 5.826558265582656,
      "grad_norm": 13.988097190856934,
      "learning_rate": 1.883468834688347e-05,
      "loss": 1.8452,
      "step": 2150
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 12.784104347229004,
      "learning_rate": 1.8829268292682926e-05,
      "loss": 1.7927,
      "step": 2160
    },
    {
      "epoch": 5.880758807588076,
      "grad_norm": 15.727721214294434,
      "learning_rate": 1.8823848238482386e-05,
      "loss": 1.7652,
      "step": 2170
    },
    {
      "epoch": 5.907859078590786,
      "grad_norm": 13.370661735534668,
      "learning_rate": 1.8818428184281846e-05,
      "loss": 1.7579,
      "step": 2180
    },
    {
      "epoch": 5.934959349593496,
      "grad_norm": 18.056617736816406,
      "learning_rate": 1.8813008130081302e-05,
      "loss": 1.8089,
      "step": 2190
    },
    {
      "epoch": 5.9620596205962055,
      "grad_norm": 15.117613792419434,
      "learning_rate": 1.8807588075880762e-05,
      "loss": 1.7406,
      "step": 2200
    },
    {
      "epoch": 5.989159891598916,
      "grad_norm": 18.656341552734375,
      "learning_rate": 1.880216802168022e-05,
      "loss": 1.8113,
      "step": 2210
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.45663956639566394,
      "eval_f1": 0.4817613537348669,
      "eval_loss": 1.991817831993103,
      "eval_runtime": 2.8343,
      "eval_samples_per_second": 260.378,
      "eval_steps_per_second": 32.812,
      "step": 2214
    },
    {
      "epoch": 6.016260162601626,
      "grad_norm": 12.685647010803223,
      "learning_rate": 1.8796747967479675e-05,
      "loss": 1.7901,
      "step": 2220
    },
    {
      "epoch": 6.043360433604336,
      "grad_norm": 7.52991247177124,
      "learning_rate": 1.8791327913279135e-05,
      "loss": 1.497,
      "step": 2230
    },
    {
      "epoch": 6.070460704607046,
      "grad_norm": 13.148506164550781,
      "learning_rate": 1.878590785907859e-05,
      "loss": 1.5972,
      "step": 2240
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 18.458026885986328,
      "learning_rate": 1.878048780487805e-05,
      "loss": 1.8032,
      "step": 2250
    },
    {
      "epoch": 6.124661246612466,
      "grad_norm": 11.821701049804688,
      "learning_rate": 1.877506775067751e-05,
      "loss": 1.5836,
      "step": 2260
    },
    {
      "epoch": 6.151761517615176,
      "grad_norm": 17.966917037963867,
      "learning_rate": 1.8769647696476967e-05,
      "loss": 1.8254,
      "step": 2270
    },
    {
      "epoch": 6.178861788617886,
      "grad_norm": 13.557832717895508,
      "learning_rate": 1.8764227642276423e-05,
      "loss": 1.8237,
      "step": 2280
    },
    {
      "epoch": 6.205962059620596,
      "grad_norm": 11.564597129821777,
      "learning_rate": 1.8758807588075883e-05,
      "loss": 1.6645,
      "step": 2290
    },
    {
      "epoch": 6.233062330623306,
      "grad_norm": 11.806733131408691,
      "learning_rate": 1.875338753387534e-05,
      "loss": 1.827,
      "step": 2300
    },
    {
      "epoch": 6.260162601626016,
      "grad_norm": 13.891738891601562,
      "learning_rate": 1.87479674796748e-05,
      "loss": 1.6702,
      "step": 2310
    },
    {
      "epoch": 6.287262872628727,
      "grad_norm": 13.962913513183594,
      "learning_rate": 1.8742547425474256e-05,
      "loss": 1.5757,
      "step": 2320
    },
    {
      "epoch": 6.314363143631437,
      "grad_norm": 10.490411758422852,
      "learning_rate": 1.8737127371273712e-05,
      "loss": 1.7238,
      "step": 2330
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 7.429293155670166,
      "learning_rate": 1.8731707317073172e-05,
      "loss": 1.657,
      "step": 2340
    },
    {
      "epoch": 6.368563685636857,
      "grad_norm": 23.657527923583984,
      "learning_rate": 1.872628726287263e-05,
      "loss": 1.8471,
      "step": 2350
    },
    {
      "epoch": 6.3956639566395665,
      "grad_norm": 11.502235412597656,
      "learning_rate": 1.8720867208672088e-05,
      "loss": 1.6391,
      "step": 2360
    },
    {
      "epoch": 6.4227642276422765,
      "grad_norm": 16.99201774597168,
      "learning_rate": 1.8715447154471548e-05,
      "loss": 1.7633,
      "step": 2370
    },
    {
      "epoch": 6.4498644986449865,
      "grad_norm": 12.792116165161133,
      "learning_rate": 1.8710027100271004e-05,
      "loss": 1.6775,
      "step": 2380
    },
    {
      "epoch": 6.476964769647696,
      "grad_norm": 22.441822052001953,
      "learning_rate": 1.870460704607046e-05,
      "loss": 1.5266,
      "step": 2390
    },
    {
      "epoch": 6.504065040650406,
      "grad_norm": 20.506362915039062,
      "learning_rate": 1.869918699186992e-05,
      "loss": 1.5621,
      "step": 2400
    },
    {
      "epoch": 6.531165311653116,
      "grad_norm": 12.127213478088379,
      "learning_rate": 1.8693766937669377e-05,
      "loss": 1.4892,
      "step": 2410
    },
    {
      "epoch": 6.558265582655826,
      "grad_norm": 15.590853691101074,
      "learning_rate": 1.8688346883468836e-05,
      "loss": 1.6848,
      "step": 2420
    },
    {
      "epoch": 6.585365853658536,
      "grad_norm": 15.290569305419922,
      "learning_rate": 1.8682926829268296e-05,
      "loss": 1.5758,
      "step": 2430
    },
    {
      "epoch": 6.612466124661246,
      "grad_norm": 12.771077156066895,
      "learning_rate": 1.8677506775067752e-05,
      "loss": 1.6551,
      "step": 2440
    },
    {
      "epoch": 6.639566395663957,
      "grad_norm": 13.07657527923584,
      "learning_rate": 1.8672086720867212e-05,
      "loss": 1.8204,
      "step": 2450
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 12.866949081420898,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.6173,
      "step": 2460
    },
    {
      "epoch": 6.693766937669377,
      "grad_norm": 9.890312194824219,
      "learning_rate": 1.8661246612466125e-05,
      "loss": 1.7094,
      "step": 2470
    },
    {
      "epoch": 6.720867208672087,
      "grad_norm": 11.514143943786621,
      "learning_rate": 1.8655826558265585e-05,
      "loss": 1.7584,
      "step": 2480
    },
    {
      "epoch": 6.747967479674797,
      "grad_norm": 18.15127944946289,
      "learning_rate": 1.865040650406504e-05,
      "loss": 1.9078,
      "step": 2490
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 11.101659774780273,
      "learning_rate": 1.86449864498645e-05,
      "loss": 1.6428,
      "step": 2500
    },
    {
      "epoch": 6.802168021680217,
      "grad_norm": 22.10956573486328,
      "learning_rate": 1.8639566395663957e-05,
      "loss": 1.5402,
      "step": 2510
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 21.808691024780273,
      "learning_rate": 1.8634146341463417e-05,
      "loss": 1.809,
      "step": 2520
    },
    {
      "epoch": 6.856368563685637,
      "grad_norm": 11.247739791870117,
      "learning_rate": 1.8628726287262874e-05,
      "loss": 1.6587,
      "step": 2530
    },
    {
      "epoch": 6.883468834688347,
      "grad_norm": 16.639812469482422,
      "learning_rate": 1.8623306233062333e-05,
      "loss": 1.6713,
      "step": 2540
    },
    {
      "epoch": 6.9105691056910565,
      "grad_norm": 12.047592163085938,
      "learning_rate": 1.861788617886179e-05,
      "loss": 1.6014,
      "step": 2550
    },
    {
      "epoch": 6.937669376693767,
      "grad_norm": 11.332128524780273,
      "learning_rate": 1.861246612466125e-05,
      "loss": 1.3329,
      "step": 2560
    },
    {
      "epoch": 6.964769647696477,
      "grad_norm": 19.63593101501465,
      "learning_rate": 1.8607046070460706e-05,
      "loss": 1.4413,
      "step": 2570
    },
    {
      "epoch": 6.991869918699187,
      "grad_norm": 14.228081703186035,
      "learning_rate": 1.8601626016260162e-05,
      "loss": 1.5992,
      "step": 2580
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.48509485094850946,
      "eval_f1": 0.5021976759472315,
      "eval_loss": 1.8871945142745972,
      "eval_runtime": 2.6323,
      "eval_samples_per_second": 280.363,
      "eval_steps_per_second": 35.33,
      "step": 2583
    },
    {
      "epoch": 7.018970189701897,
      "grad_norm": 11.900567054748535,
      "learning_rate": 1.8596205962059622e-05,
      "loss": 1.573,
      "step": 2590
    },
    {
      "epoch": 7.046070460704607,
      "grad_norm": 12.90783977508545,
      "learning_rate": 1.859078590785908e-05,
      "loss": 1.464,
      "step": 2600
    },
    {
      "epoch": 7.073170731707317,
      "grad_norm": 8.983480453491211,
      "learning_rate": 1.8585365853658538e-05,
      "loss": 1.2837,
      "step": 2610
    },
    {
      "epoch": 7.100271002710027,
      "grad_norm": 12.257599830627441,
      "learning_rate": 1.8579945799457998e-05,
      "loss": 1.6972,
      "step": 2620
    },
    {
      "epoch": 7.127371273712737,
      "grad_norm": 14.388420104980469,
      "learning_rate": 1.8574525745257454e-05,
      "loss": 1.5938,
      "step": 2630
    },
    {
      "epoch": 7.154471544715447,
      "grad_norm": 14.470901489257812,
      "learning_rate": 1.856910569105691e-05,
      "loss": 1.3452,
      "step": 2640
    },
    {
      "epoch": 7.181571815718157,
      "grad_norm": 15.795267105102539,
      "learning_rate": 1.856368563685637e-05,
      "loss": 1.6806,
      "step": 2650
    },
    {
      "epoch": 7.208672086720867,
      "grad_norm": 19.40306854248047,
      "learning_rate": 1.8558265582655827e-05,
      "loss": 1.4856,
      "step": 2660
    },
    {
      "epoch": 7.235772357723577,
      "grad_norm": 13.240738868713379,
      "learning_rate": 1.8552845528455287e-05,
      "loss": 1.3409,
      "step": 2670
    },
    {
      "epoch": 7.262872628726287,
      "grad_norm": 11.090518951416016,
      "learning_rate": 1.8547425474254743e-05,
      "loss": 1.4055,
      "step": 2680
    },
    {
      "epoch": 7.289972899728998,
      "grad_norm": 15.409852981567383,
      "learning_rate": 1.85420054200542e-05,
      "loss": 1.2972,
      "step": 2690
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 12.716798782348633,
      "learning_rate": 1.8536585365853663e-05,
      "loss": 1.6609,
      "step": 2700
    },
    {
      "epoch": 7.3441734417344176,
      "grad_norm": 9.16104793548584,
      "learning_rate": 1.853116531165312e-05,
      "loss": 1.7303,
      "step": 2710
    },
    {
      "epoch": 7.3712737127371275,
      "grad_norm": 17.4032039642334,
      "learning_rate": 1.8525745257452575e-05,
      "loss": 1.6646,
      "step": 2720
    },
    {
      "epoch": 7.3983739837398375,
      "grad_norm": 21.564411163330078,
      "learning_rate": 1.8520325203252035e-05,
      "loss": 1.4114,
      "step": 2730
    },
    {
      "epoch": 7.425474254742547,
      "grad_norm": 13.172133445739746,
      "learning_rate": 1.851490514905149e-05,
      "loss": 1.3952,
      "step": 2740
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 10.645469665527344,
      "learning_rate": 1.850948509485095e-05,
      "loss": 1.3121,
      "step": 2750
    },
    {
      "epoch": 7.479674796747967,
      "grad_norm": 22.35701560974121,
      "learning_rate": 1.8504065040650408e-05,
      "loss": 1.5382,
      "step": 2760
    },
    {
      "epoch": 7.506775067750677,
      "grad_norm": 12.175379753112793,
      "learning_rate": 1.8498644986449864e-05,
      "loss": 1.7641,
      "step": 2770
    },
    {
      "epoch": 7.533875338753387,
      "grad_norm": 14.536158561706543,
      "learning_rate": 1.8493224932249324e-05,
      "loss": 1.3887,
      "step": 2780
    },
    {
      "epoch": 7.560975609756097,
      "grad_norm": 9.990668296813965,
      "learning_rate": 1.8487804878048784e-05,
      "loss": 1.364,
      "step": 2790
    },
    {
      "epoch": 7.588075880758808,
      "grad_norm": 13.79257583618164,
      "learning_rate": 1.848238482384824e-05,
      "loss": 1.6132,
      "step": 2800
    },
    {
      "epoch": 7.615176151761518,
      "grad_norm": 10.798802375793457,
      "learning_rate": 1.84769647696477e-05,
      "loss": 1.5259,
      "step": 2810
    },
    {
      "epoch": 7.642276422764228,
      "grad_norm": 17.1517276763916,
      "learning_rate": 1.8471544715447156e-05,
      "loss": 1.6519,
      "step": 2820
    },
    {
      "epoch": 7.669376693766938,
      "grad_norm": 14.126791954040527,
      "learning_rate": 1.8466124661246612e-05,
      "loss": 1.33,
      "step": 2830
    },
    {
      "epoch": 7.696476964769648,
      "grad_norm": 12.24920654296875,
      "learning_rate": 1.8460704607046072e-05,
      "loss": 1.7012,
      "step": 2840
    },
    {
      "epoch": 7.723577235772358,
      "grad_norm": 10.868537902832031,
      "learning_rate": 1.845528455284553e-05,
      "loss": 1.5317,
      "step": 2850
    },
    {
      "epoch": 7.750677506775068,
      "grad_norm": 14.66146469116211,
      "learning_rate": 1.844986449864499e-05,
      "loss": 1.345,
      "step": 2860
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 12.226617813110352,
      "learning_rate": 1.8444444444444448e-05,
      "loss": 1.2811,
      "step": 2870
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 14.749229431152344,
      "learning_rate": 1.8439024390243905e-05,
      "loss": 1.7429,
      "step": 2880
    },
    {
      "epoch": 7.831978319783198,
      "grad_norm": 14.625731468200684,
      "learning_rate": 1.843360433604336e-05,
      "loss": 1.5948,
      "step": 2890
    },
    {
      "epoch": 7.8590785907859075,
      "grad_norm": 13.882556915283203,
      "learning_rate": 1.842818428184282e-05,
      "loss": 1.2813,
      "step": 2900
    },
    {
      "epoch": 7.886178861788618,
      "grad_norm": 16.58664321899414,
      "learning_rate": 1.8422764227642277e-05,
      "loss": 1.2709,
      "step": 2910
    },
    {
      "epoch": 7.913279132791327,
      "grad_norm": 18.351591110229492,
      "learning_rate": 1.8417344173441737e-05,
      "loss": 1.7976,
      "step": 2920
    },
    {
      "epoch": 7.940379403794038,
      "grad_norm": 16.010061264038086,
      "learning_rate": 1.8411924119241193e-05,
      "loss": 1.1275,
      "step": 2930
    },
    {
      "epoch": 7.967479674796748,
      "grad_norm": 10.564573287963867,
      "learning_rate": 1.840650406504065e-05,
      "loss": 1.4683,
      "step": 2940
    },
    {
      "epoch": 7.994579945799458,
      "grad_norm": 18.078760147094727,
      "learning_rate": 1.840108401084011e-05,
      "loss": 1.5296,
      "step": 2950
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.494579945799458,
      "eval_f1": 0.522597070666082,
      "eval_loss": 1.8722658157348633,
      "eval_runtime": 2.7418,
      "eval_samples_per_second": 269.165,
      "eval_steps_per_second": 33.919,
      "step": 2952
    },
    {
      "epoch": 8.021680216802167,
      "grad_norm": 16.589441299438477,
      "learning_rate": 1.839566395663957e-05,
      "loss": 1.5798,
      "step": 2960
    },
    {
      "epoch": 8.048780487804878,
      "grad_norm": 10.37359619140625,
      "learning_rate": 1.8390243902439026e-05,
      "loss": 1.3826,
      "step": 2970
    },
    {
      "epoch": 8.075880758807589,
      "grad_norm": 17.462448120117188,
      "learning_rate": 1.8384823848238485e-05,
      "loss": 1.2246,
      "step": 2980
    },
    {
      "epoch": 8.102981029810298,
      "grad_norm": 6.089468002319336,
      "learning_rate": 1.8379403794037942e-05,
      "loss": 1.5275,
      "step": 2990
    },
    {
      "epoch": 8.130081300813009,
      "grad_norm": 15.453170776367188,
      "learning_rate": 1.83739837398374e-05,
      "loss": 1.2658,
      "step": 3000
    },
    {
      "epoch": 8.157181571815718,
      "grad_norm": 10.369872093200684,
      "learning_rate": 1.8368563685636858e-05,
      "loss": 0.9801,
      "step": 3010
    },
    {
      "epoch": 8.184281842818429,
      "grad_norm": 13.490565299987793,
      "learning_rate": 1.8363143631436314e-05,
      "loss": 1.1764,
      "step": 3020
    },
    {
      "epoch": 8.211382113821138,
      "grad_norm": 16.999937057495117,
      "learning_rate": 1.8357723577235774e-05,
      "loss": 1.5719,
      "step": 3030
    },
    {
      "epoch": 8.238482384823849,
      "grad_norm": 7.926289081573486,
      "learning_rate": 1.835230352303523e-05,
      "loss": 1.3387,
      "step": 3040
    },
    {
      "epoch": 8.265582655826558,
      "grad_norm": 13.124385833740234,
      "learning_rate": 1.834688346883469e-05,
      "loss": 1.2548,
      "step": 3050
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 14.052780151367188,
      "learning_rate": 1.834146341463415e-05,
      "loss": 1.4954,
      "step": 3060
    },
    {
      "epoch": 8.319783197831978,
      "grad_norm": 11.618675231933594,
      "learning_rate": 1.8336043360433606e-05,
      "loss": 1.5343,
      "step": 3070
    },
    {
      "epoch": 8.346883468834688,
      "grad_norm": 12.159340858459473,
      "learning_rate": 1.8330623306233063e-05,
      "loss": 1.1046,
      "step": 3080
    },
    {
      "epoch": 8.373983739837398,
      "grad_norm": 17.65424919128418,
      "learning_rate": 1.8325203252032523e-05,
      "loss": 1.4039,
      "step": 3090
    },
    {
      "epoch": 8.401084010840108,
      "grad_norm": 17.337020874023438,
      "learning_rate": 1.831978319783198e-05,
      "loss": 1.4533,
      "step": 3100
    },
    {
      "epoch": 8.42818428184282,
      "grad_norm": 16.1328125,
      "learning_rate": 1.831436314363144e-05,
      "loss": 1.3415,
      "step": 3110
    },
    {
      "epoch": 8.455284552845528,
      "grad_norm": 12.641676902770996,
      "learning_rate": 1.8308943089430895e-05,
      "loss": 1.2423,
      "step": 3120
    },
    {
      "epoch": 8.482384823848239,
      "grad_norm": 18.916418075561523,
      "learning_rate": 1.830352303523035e-05,
      "loss": 1.4816,
      "step": 3130
    },
    {
      "epoch": 8.509485094850948,
      "grad_norm": 11.714452743530273,
      "learning_rate": 1.829810298102981e-05,
      "loss": 1.5771,
      "step": 3140
    },
    {
      "epoch": 8.536585365853659,
      "grad_norm": 11.851616859436035,
      "learning_rate": 1.829268292682927e-05,
      "loss": 1.5474,
      "step": 3150
    },
    {
      "epoch": 8.563685636856368,
      "grad_norm": 20.222129821777344,
      "learning_rate": 1.8287262872628727e-05,
      "loss": 1.4839,
      "step": 3160
    },
    {
      "epoch": 8.590785907859079,
      "grad_norm": 7.548581600189209,
      "learning_rate": 1.8281842818428187e-05,
      "loss": 1.3967,
      "step": 3170
    },
    {
      "epoch": 8.617886178861788,
      "grad_norm": 30.185148239135742,
      "learning_rate": 1.8276422764227644e-05,
      "loss": 1.2288,
      "step": 3180
    },
    {
      "epoch": 8.644986449864499,
      "grad_norm": 12.431293487548828,
      "learning_rate": 1.82710027100271e-05,
      "loss": 1.3471,
      "step": 3190
    },
    {
      "epoch": 8.672086720867208,
      "grad_norm": 13.573481559753418,
      "learning_rate": 1.826558265582656e-05,
      "loss": 1.2962,
      "step": 3200
    },
    {
      "epoch": 8.699186991869919,
      "grad_norm": 11.675246238708496,
      "learning_rate": 1.8260162601626016e-05,
      "loss": 1.1153,
      "step": 3210
    },
    {
      "epoch": 8.726287262872628,
      "grad_norm": 11.04387092590332,
      "learning_rate": 1.8254742547425476e-05,
      "loss": 1.1768,
      "step": 3220
    },
    {
      "epoch": 8.753387533875339,
      "grad_norm": 23.757339477539062,
      "learning_rate": 1.8249322493224936e-05,
      "loss": 1.4975,
      "step": 3230
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 16.12346649169922,
      "learning_rate": 1.8243902439024392e-05,
      "loss": 1.163,
      "step": 3240
    },
    {
      "epoch": 8.807588075880759,
      "grad_norm": 12.355975151062012,
      "learning_rate": 1.8238482384823852e-05,
      "loss": 1.453,
      "step": 3250
    },
    {
      "epoch": 8.83468834688347,
      "grad_norm": 12.154886245727539,
      "learning_rate": 1.8233062330623308e-05,
      "loss": 1.4404,
      "step": 3260
    },
    {
      "epoch": 8.861788617886178,
      "grad_norm": 6.082998752593994,
      "learning_rate": 1.8227642276422765e-05,
      "loss": 1.1954,
      "step": 3270
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 16.74783706665039,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 1.5373,
      "step": 3280
    },
    {
      "epoch": 8.915989159891598,
      "grad_norm": 18.260385513305664,
      "learning_rate": 1.821680216802168e-05,
      "loss": 1.6404,
      "step": 3290
    },
    {
      "epoch": 8.94308943089431,
      "grad_norm": 7.981229305267334,
      "learning_rate": 1.821138211382114e-05,
      "loss": 0.9424,
      "step": 3300
    },
    {
      "epoch": 8.970189701897018,
      "grad_norm": 10.868996620178223,
      "learning_rate": 1.8205962059620597e-05,
      "loss": 1.57,
      "step": 3310
    },
    {
      "epoch": 8.997289972899729,
      "grad_norm": 11.531288146972656,
      "learning_rate": 1.8200542005420057e-05,
      "loss": 1.238,
      "step": 3320
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.5,
      "eval_f1": 0.5220713671719354,
      "eval_loss": 1.8257267475128174,
      "eval_runtime": 2.7541,
      "eval_samples_per_second": 267.963,
      "eval_steps_per_second": 33.768,
      "step": 3321
    },
    {
      "epoch": 9.024390243902438,
      "grad_norm": 11.891854286193848,
      "learning_rate": 1.8195121951219513e-05,
      "loss": 1.4711,
      "step": 3330
    },
    {
      "epoch": 9.051490514905149,
      "grad_norm": 14.985224723815918,
      "learning_rate": 1.8189701897018973e-05,
      "loss": 1.546,
      "step": 3340
    },
    {
      "epoch": 9.07859078590786,
      "grad_norm": 10.783157348632812,
      "learning_rate": 1.818428184281843e-05,
      "loss": 1.3287,
      "step": 3350
    },
    {
      "epoch": 9.105691056910569,
      "grad_norm": 17.881868362426758,
      "learning_rate": 1.817886178861789e-05,
      "loss": 1.1379,
      "step": 3360
    },
    {
      "epoch": 9.13279132791328,
      "grad_norm": 10.706138610839844,
      "learning_rate": 1.8173441734417345e-05,
      "loss": 1.295,
      "step": 3370
    },
    {
      "epoch": 9.159891598915989,
      "grad_norm": 15.731492042541504,
      "learning_rate": 1.8168021680216802e-05,
      "loss": 1.1163,
      "step": 3380
    },
    {
      "epoch": 9.1869918699187,
      "grad_norm": 20.568483352661133,
      "learning_rate": 1.816260162601626e-05,
      "loss": 1.2141,
      "step": 3390
    },
    {
      "epoch": 9.214092140921409,
      "grad_norm": 9.188653945922852,
      "learning_rate": 1.815718157181572e-05,
      "loss": 1.2806,
      "step": 3400
    },
    {
      "epoch": 9.24119241192412,
      "grad_norm": 11.39599323272705,
      "learning_rate": 1.8151761517615178e-05,
      "loss": 1.4404,
      "step": 3410
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 10.402069091796875,
      "learning_rate": 1.8146341463414637e-05,
      "loss": 1.1057,
      "step": 3420
    },
    {
      "epoch": 9.29539295392954,
      "grad_norm": 10.462646484375,
      "learning_rate": 1.8140921409214094e-05,
      "loss": 1.0615,
      "step": 3430
    },
    {
      "epoch": 9.322493224932249,
      "grad_norm": 12.582003593444824,
      "learning_rate": 1.813550135501355e-05,
      "loss": 1.1473,
      "step": 3440
    },
    {
      "epoch": 9.34959349593496,
      "grad_norm": 21.570568084716797,
      "learning_rate": 1.813008130081301e-05,
      "loss": 1.516,
      "step": 3450
    },
    {
      "epoch": 9.37669376693767,
      "grad_norm": 11.170591354370117,
      "learning_rate": 1.8124661246612466e-05,
      "loss": 1.2062,
      "step": 3460
    },
    {
      "epoch": 9.40379403794038,
      "grad_norm": 10.43671703338623,
      "learning_rate": 1.8119241192411926e-05,
      "loss": 1.091,
      "step": 3470
    },
    {
      "epoch": 9.43089430894309,
      "grad_norm": 16.894357681274414,
      "learning_rate": 1.8113821138211382e-05,
      "loss": 1.3396,
      "step": 3480
    },
    {
      "epoch": 9.4579945799458,
      "grad_norm": 11.93393611907959,
      "learning_rate": 1.8108401084010842e-05,
      "loss": 1.2086,
      "step": 3490
    },
    {
      "epoch": 9.48509485094851,
      "grad_norm": 9.625929832458496,
      "learning_rate": 1.8102981029810302e-05,
      "loss": 0.9322,
      "step": 3500
    },
    {
      "epoch": 9.512195121951219,
      "grad_norm": 10.91203784942627,
      "learning_rate": 1.809756097560976e-05,
      "loss": 1.1254,
      "step": 3510
    },
    {
      "epoch": 9.53929539295393,
      "grad_norm": 9.334943771362305,
      "learning_rate": 1.8092140921409215e-05,
      "loss": 1.0796,
      "step": 3520
    },
    {
      "epoch": 9.566395663956639,
      "grad_norm": 19.19837760925293,
      "learning_rate": 1.8086720867208675e-05,
      "loss": 1.2705,
      "step": 3530
    },
    {
      "epoch": 9.59349593495935,
      "grad_norm": 12.635653495788574,
      "learning_rate": 1.808130081300813e-05,
      "loss": 1.5131,
      "step": 3540
    },
    {
      "epoch": 9.620596205962059,
      "grad_norm": 12.850833892822266,
      "learning_rate": 1.807588075880759e-05,
      "loss": 1.3428,
      "step": 3550
    },
    {
      "epoch": 9.64769647696477,
      "grad_norm": 20.791473388671875,
      "learning_rate": 1.8070460704607047e-05,
      "loss": 1.112,
      "step": 3560
    },
    {
      "epoch": 9.67479674796748,
      "grad_norm": 13.844645500183105,
      "learning_rate": 1.8065040650406504e-05,
      "loss": 1.1511,
      "step": 3570
    },
    {
      "epoch": 9.70189701897019,
      "grad_norm": 10.14445972442627,
      "learning_rate": 1.8059620596205963e-05,
      "loss": 1.505,
      "step": 3580
    },
    {
      "epoch": 9.7289972899729,
      "grad_norm": 20.216663360595703,
      "learning_rate": 1.8054200542005423e-05,
      "loss": 1.4929,
      "step": 3590
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 9.581521987915039,
      "learning_rate": 1.804878048780488e-05,
      "loss": 1.33,
      "step": 3600
    },
    {
      "epoch": 9.78319783197832,
      "grad_norm": 11.604736328125,
      "learning_rate": 1.804336043360434e-05,
      "loss": 1.2976,
      "step": 3610
    },
    {
      "epoch": 9.81029810298103,
      "grad_norm": 12.668503761291504,
      "learning_rate": 1.8037940379403796e-05,
      "loss": 1.2999,
      "step": 3620
    },
    {
      "epoch": 9.83739837398374,
      "grad_norm": 15.585494995117188,
      "learning_rate": 1.8032520325203252e-05,
      "loss": 1.142,
      "step": 3630
    },
    {
      "epoch": 9.86449864498645,
      "grad_norm": 14.3240385055542,
      "learning_rate": 1.8027100271002712e-05,
      "loss": 1.3941,
      "step": 3640
    },
    {
      "epoch": 9.89159891598916,
      "grad_norm": 11.225835800170898,
      "learning_rate": 1.8021680216802168e-05,
      "loss": 1.2071,
      "step": 3650
    },
    {
      "epoch": 9.91869918699187,
      "grad_norm": 17.6442928314209,
      "learning_rate": 1.8016260162601628e-05,
      "loss": 1.5886,
      "step": 3660
    },
    {
      "epoch": 9.94579945799458,
      "grad_norm": 12.707391738891602,
      "learning_rate": 1.8010840108401088e-05,
      "loss": 1.125,
      "step": 3670
    },
    {
      "epoch": 9.97289972899729,
      "grad_norm": 13.768106460571289,
      "learning_rate": 1.8005420054200544e-05,
      "loss": 1.3645,
      "step": 3680
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.068861961364746,
      "learning_rate": 1.8e-05,
      "loss": 1.1833,
      "step": 3690
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.505420054200542,
      "eval_f1": 0.5248411712994059,
      "eval_loss": 1.8419749736785889,
      "eval_runtime": 2.6749,
      "eval_samples_per_second": 275.895,
      "eval_steps_per_second": 34.767,
      "step": 3690
    },
    {
      "epoch": 10.02710027100271,
      "grad_norm": 10.133356094360352,
      "learning_rate": 1.799457994579946e-05,
      "loss": 1.2729,
      "step": 3700
    },
    {
      "epoch": 10.05420054200542,
      "grad_norm": 17.434478759765625,
      "learning_rate": 1.7989159891598917e-05,
      "loss": 1.0596,
      "step": 3710
    },
    {
      "epoch": 10.08130081300813,
      "grad_norm": 12.815020561218262,
      "learning_rate": 1.7983739837398376e-05,
      "loss": 1.1638,
      "step": 3720
    },
    {
      "epoch": 10.10840108401084,
      "grad_norm": 8.704275131225586,
      "learning_rate": 1.7978319783197833e-05,
      "loss": 1.375,
      "step": 3730
    },
    {
      "epoch": 10.13550135501355,
      "grad_norm": 14.196958541870117,
      "learning_rate": 1.797289972899729e-05,
      "loss": 1.0449,
      "step": 3740
    },
    {
      "epoch": 10.16260162601626,
      "grad_norm": 15.055098533630371,
      "learning_rate": 1.796747967479675e-05,
      "loss": 1.0665,
      "step": 3750
    },
    {
      "epoch": 10.18970189701897,
      "grad_norm": 12.029010772705078,
      "learning_rate": 1.796205962059621e-05,
      "loss": 1.2753,
      "step": 3760
    },
    {
      "epoch": 10.21680216802168,
      "grad_norm": 9.651872634887695,
      "learning_rate": 1.7956639566395665e-05,
      "loss": 0.9226,
      "step": 3770
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 13.849452018737793,
      "learning_rate": 1.7951219512195125e-05,
      "loss": 1.0807,
      "step": 3780
    },
    {
      "epoch": 10.2710027100271,
      "grad_norm": 13.270652770996094,
      "learning_rate": 1.794579945799458e-05,
      "loss": 1.1334,
      "step": 3790
    },
    {
      "epoch": 10.29810298102981,
      "grad_norm": 17.930177688598633,
      "learning_rate": 1.7940379403794038e-05,
      "loss": 1.0618,
      "step": 3800
    },
    {
      "epoch": 10.32520325203252,
      "grad_norm": 3.5709524154663086,
      "learning_rate": 1.7934959349593497e-05,
      "loss": 1.1953,
      "step": 3810
    },
    {
      "epoch": 10.35230352303523,
      "grad_norm": 13.411029815673828,
      "learning_rate": 1.7929539295392954e-05,
      "loss": 1.2131,
      "step": 3820
    },
    {
      "epoch": 10.379403794037941,
      "grad_norm": 8.966830253601074,
      "learning_rate": 1.7924119241192414e-05,
      "loss": 0.982,
      "step": 3830
    },
    {
      "epoch": 10.40650406504065,
      "grad_norm": 21.52105712890625,
      "learning_rate": 1.791869918699187e-05,
      "loss": 1.1522,
      "step": 3840
    },
    {
      "epoch": 10.433604336043361,
      "grad_norm": 4.467576503753662,
      "learning_rate": 1.791327913279133e-05,
      "loss": 1.0229,
      "step": 3850
    },
    {
      "epoch": 10.46070460704607,
      "grad_norm": 14.918357849121094,
      "learning_rate": 1.790785907859079e-05,
      "loss": 1.2859,
      "step": 3860
    },
    {
      "epoch": 10.487804878048781,
      "grad_norm": 9.036044120788574,
      "learning_rate": 1.7902439024390246e-05,
      "loss": 1.0234,
      "step": 3870
    },
    {
      "epoch": 10.51490514905149,
      "grad_norm": 8.210742950439453,
      "learning_rate": 1.7897018970189702e-05,
      "loss": 1.2676,
      "step": 3880
    },
    {
      "epoch": 10.5420054200542,
      "grad_norm": 23.31695556640625,
      "learning_rate": 1.7891598915989162e-05,
      "loss": 1.6542,
      "step": 3890
    },
    {
      "epoch": 10.56910569105691,
      "grad_norm": 11.83764934539795,
      "learning_rate": 1.788617886178862e-05,
      "loss": 1.1253,
      "step": 3900
    },
    {
      "epoch": 10.59620596205962,
      "grad_norm": 14.465752601623535,
      "learning_rate": 1.7880758807588078e-05,
      "loss": 1.35,
      "step": 3910
    },
    {
      "epoch": 10.62330623306233,
      "grad_norm": 13.477852821350098,
      "learning_rate": 1.7875338753387535e-05,
      "loss": 1.2508,
      "step": 3920
    },
    {
      "epoch": 10.65040650406504,
      "grad_norm": 20.965103149414062,
      "learning_rate": 1.7869918699186994e-05,
      "loss": 1.6225,
      "step": 3930
    },
    {
      "epoch": 10.677506775067751,
      "grad_norm": 13.68958854675293,
      "learning_rate": 1.786449864498645e-05,
      "loss": 1.0013,
      "step": 3940
    },
    {
      "epoch": 10.70460704607046,
      "grad_norm": 14.57158374786377,
      "learning_rate": 1.785907859078591e-05,
      "loss": 1.0336,
      "step": 3950
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 28.155330657958984,
      "learning_rate": 1.7853658536585367e-05,
      "loss": 1.1616,
      "step": 3960
    },
    {
      "epoch": 10.75880758807588,
      "grad_norm": 10.838242530822754,
      "learning_rate": 1.7848238482384827e-05,
      "loss": 1.0359,
      "step": 3970
    },
    {
      "epoch": 10.785907859078591,
      "grad_norm": 11.61544132232666,
      "learning_rate": 1.7842818428184283e-05,
      "loss": 1.2777,
      "step": 3980
    },
    {
      "epoch": 10.8130081300813,
      "grad_norm": 12.45083236694336,
      "learning_rate": 1.783739837398374e-05,
      "loss": 1.1202,
      "step": 3990
    },
    {
      "epoch": 10.840108401084011,
      "grad_norm": 12.10009765625,
      "learning_rate": 1.78319783197832e-05,
      "loss": 1.1568,
      "step": 4000
    },
    {
      "epoch": 10.86720867208672,
      "grad_norm": 8.754822731018066,
      "learning_rate": 1.7826558265582656e-05,
      "loss": 1.1677,
      "step": 4010
    },
    {
      "epoch": 10.894308943089431,
      "grad_norm": 14.160801887512207,
      "learning_rate": 1.7821138211382115e-05,
      "loss": 1.3584,
      "step": 4020
    },
    {
      "epoch": 10.92140921409214,
      "grad_norm": 14.434216499328613,
      "learning_rate": 1.7815718157181575e-05,
      "loss": 1.4471,
      "step": 4030
    },
    {
      "epoch": 10.948509485094851,
      "grad_norm": 5.189041614532471,
      "learning_rate": 1.781029810298103e-05,
      "loss": 1.2124,
      "step": 4040
    },
    {
      "epoch": 10.975609756097562,
      "grad_norm": 16.44841194152832,
      "learning_rate": 1.7804878048780488e-05,
      "loss": 1.1547,
      "step": 4050
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.524390243902439,
      "eval_f1": 0.5401232646486533,
      "eval_loss": 1.8118606805801392,
      "eval_runtime": 2.7139,
      "eval_samples_per_second": 271.935,
      "eval_steps_per_second": 34.268,
      "step": 4059
    },
    {
      "epoch": 11.002710027100271,
      "grad_norm": 6.235743045806885,
      "learning_rate": 1.7799457994579948e-05,
      "loss": 1.3549,
      "step": 4060
    },
    {
      "epoch": 11.029810298102982,
      "grad_norm": 7.348677635192871,
      "learning_rate": 1.7794037940379404e-05,
      "loss": 1.1788,
      "step": 4070
    },
    {
      "epoch": 11.05691056910569,
      "grad_norm": 16.50979995727539,
      "learning_rate": 1.7788617886178864e-05,
      "loss": 1.1611,
      "step": 4080
    },
    {
      "epoch": 11.084010840108402,
      "grad_norm": 3.7363390922546387,
      "learning_rate": 1.778319783197832e-05,
      "loss": 0.7849,
      "step": 4090
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 11.357545852661133,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 1.1865,
      "step": 4100
    },
    {
      "epoch": 11.138211382113822,
      "grad_norm": 20.027320861816406,
      "learning_rate": 1.777235772357724e-05,
      "loss": 0.9762,
      "step": 4110
    },
    {
      "epoch": 11.16531165311653,
      "grad_norm": 11.275931358337402,
      "learning_rate": 1.7766937669376696e-05,
      "loss": 1.3144,
      "step": 4120
    },
    {
      "epoch": 11.192411924119241,
      "grad_norm": 13.292296409606934,
      "learning_rate": 1.7761517615176153e-05,
      "loss": 1.1151,
      "step": 4130
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 8.63039493560791,
      "learning_rate": 1.7756097560975612e-05,
      "loss": 1.2888,
      "step": 4140
    },
    {
      "epoch": 11.246612466124661,
      "grad_norm": 11.083013534545898,
      "learning_rate": 1.775067750677507e-05,
      "loss": 1.1157,
      "step": 4150
    },
    {
      "epoch": 11.27371273712737,
      "grad_norm": 8.782327651977539,
      "learning_rate": 1.774525745257453e-05,
      "loss": 1.3707,
      "step": 4160
    },
    {
      "epoch": 11.300813008130081,
      "grad_norm": 15.113200187683105,
      "learning_rate": 1.7739837398373985e-05,
      "loss": 1.0243,
      "step": 4170
    },
    {
      "epoch": 11.327913279132792,
      "grad_norm": 14.819358825683594,
      "learning_rate": 1.773441734417344e-05,
      "loss": 1.057,
      "step": 4180
    },
    {
      "epoch": 11.355013550135501,
      "grad_norm": 12.350617408752441,
      "learning_rate": 1.77289972899729e-05,
      "loss": 1.2889,
      "step": 4190
    },
    {
      "epoch": 11.382113821138212,
      "grad_norm": 11.217583656311035,
      "learning_rate": 1.772357723577236e-05,
      "loss": 1.1183,
      "step": 4200
    },
    {
      "epoch": 11.409214092140921,
      "grad_norm": 8.335282325744629,
      "learning_rate": 1.7718157181571817e-05,
      "loss": 0.7811,
      "step": 4210
    },
    {
      "epoch": 11.436314363143632,
      "grad_norm": 15.904885292053223,
      "learning_rate": 1.7712737127371277e-05,
      "loss": 1.1307,
      "step": 4220
    },
    {
      "epoch": 11.463414634146341,
      "grad_norm": 10.435154914855957,
      "learning_rate": 1.7707317073170733e-05,
      "loss": 1.4538,
      "step": 4230
    },
    {
      "epoch": 11.490514905149052,
      "grad_norm": 11.894207000732422,
      "learning_rate": 1.770189701897019e-05,
      "loss": 1.0901,
      "step": 4240
    },
    {
      "epoch": 11.517615176151761,
      "grad_norm": 13.984822273254395,
      "learning_rate": 1.769647696476965e-05,
      "loss": 0.988,
      "step": 4250
    },
    {
      "epoch": 11.544715447154472,
      "grad_norm": 20.665616989135742,
      "learning_rate": 1.7691056910569106e-05,
      "loss": 1.1142,
      "step": 4260
    },
    {
      "epoch": 11.57181571815718,
      "grad_norm": 14.757464408874512,
      "learning_rate": 1.7685636856368566e-05,
      "loss": 0.9807,
      "step": 4270
    },
    {
      "epoch": 11.598915989159892,
      "grad_norm": 16.22480583190918,
      "learning_rate": 1.7680216802168022e-05,
      "loss": 1.2428,
      "step": 4280
    },
    {
      "epoch": 11.6260162601626,
      "grad_norm": 11.738290786743164,
      "learning_rate": 1.7674796747967482e-05,
      "loss": 0.997,
      "step": 4290
    },
    {
      "epoch": 11.653116531165312,
      "grad_norm": 13.105463981628418,
      "learning_rate": 1.7669376693766938e-05,
      "loss": 1.2271,
      "step": 4300
    },
    {
      "epoch": 11.680216802168022,
      "grad_norm": 11.640724182128906,
      "learning_rate": 1.7663956639566398e-05,
      "loss": 1.1596,
      "step": 4310
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 16.771764755249023,
      "learning_rate": 1.7658536585365854e-05,
      "loss": 1.1132,
      "step": 4320
    },
    {
      "epoch": 11.734417344173442,
      "grad_norm": 9.008377075195312,
      "learning_rate": 1.7653116531165314e-05,
      "loss": 1.4166,
      "step": 4330
    },
    {
      "epoch": 11.761517615176151,
      "grad_norm": 14.659896850585938,
      "learning_rate": 1.764769647696477e-05,
      "loss": 0.9197,
      "step": 4340
    },
    {
      "epoch": 11.788617886178862,
      "grad_norm": 21.887357711791992,
      "learning_rate": 1.7642276422764227e-05,
      "loss": 1.096,
      "step": 4350
    },
    {
      "epoch": 11.815718157181571,
      "grad_norm": 6.564675807952881,
      "learning_rate": 1.7636856368563687e-05,
      "loss": 0.9978,
      "step": 4360
    },
    {
      "epoch": 11.842818428184282,
      "grad_norm": 14.213772773742676,
      "learning_rate": 1.7631436314363146e-05,
      "loss": 1.1077,
      "step": 4370
    },
    {
      "epoch": 11.869918699186991,
      "grad_norm": 9.949893951416016,
      "learning_rate": 1.7626016260162603e-05,
      "loss": 1.1111,
      "step": 4380
    },
    {
      "epoch": 11.897018970189702,
      "grad_norm": 17.0465145111084,
      "learning_rate": 1.7620596205962063e-05,
      "loss": 1.065,
      "step": 4390
    },
    {
      "epoch": 11.924119241192411,
      "grad_norm": 13.14262866973877,
      "learning_rate": 1.761517615176152e-05,
      "loss": 1.4646,
      "step": 4400
    },
    {
      "epoch": 11.951219512195122,
      "grad_norm": 18.78888702392578,
      "learning_rate": 1.760975609756098e-05,
      "loss": 1.0247,
      "step": 4410
    },
    {
      "epoch": 11.978319783197833,
      "grad_norm": 13.250199317932129,
      "learning_rate": 1.7604336043360435e-05,
      "loss": 1.1858,
      "step": 4420
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.524390243902439,
      "eval_f1": 0.5410036723972644,
      "eval_loss": 1.8084299564361572,
      "eval_runtime": 2.7171,
      "eval_samples_per_second": 271.612,
      "eval_steps_per_second": 34.228,
      "step": 4428
    },
    {
      "epoch": 12.005420054200542,
      "grad_norm": 5.3943023681640625,
      "learning_rate": 1.759891598915989e-05,
      "loss": 1.0484,
      "step": 4430
    },
    {
      "epoch": 12.032520325203253,
      "grad_norm": 8.738832473754883,
      "learning_rate": 1.759349593495935e-05,
      "loss": 1.0817,
      "step": 4440
    },
    {
      "epoch": 12.059620596205962,
      "grad_norm": 13.469898223876953,
      "learning_rate": 1.7588075880758808e-05,
      "loss": 1.0018,
      "step": 4450
    },
    {
      "epoch": 12.086720867208673,
      "grad_norm": 11.60140609741211,
      "learning_rate": 1.7582655826558267e-05,
      "loss": 1.0153,
      "step": 4460
    },
    {
      "epoch": 12.113821138211382,
      "grad_norm": 9.676475524902344,
      "learning_rate": 1.7577235772357727e-05,
      "loss": 1.1656,
      "step": 4470
    },
    {
      "epoch": 12.140921409214092,
      "grad_norm": 9.953646659851074,
      "learning_rate": 1.7571815718157184e-05,
      "loss": 1.3019,
      "step": 4480
    },
    {
      "epoch": 12.168021680216802,
      "grad_norm": 6.6056623458862305,
      "learning_rate": 1.756639566395664e-05,
      "loss": 0.9184,
      "step": 4490
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 11.851205825805664,
      "learning_rate": 1.75609756097561e-05,
      "loss": 1.4343,
      "step": 4500
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 5.460727214813232,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 0.8337,
      "step": 4510
    },
    {
      "epoch": 12.249322493224932,
      "grad_norm": 13.495141983032227,
      "learning_rate": 1.7550135501355016e-05,
      "loss": 0.869,
      "step": 4520
    },
    {
      "epoch": 12.276422764227643,
      "grad_norm": 19.380176544189453,
      "learning_rate": 1.7544715447154472e-05,
      "loss": 1.1529,
      "step": 4530
    },
    {
      "epoch": 12.303523035230352,
      "grad_norm": 10.931468963623047,
      "learning_rate": 1.753929539295393e-05,
      "loss": 1.1476,
      "step": 4540
    },
    {
      "epoch": 12.330623306233063,
      "grad_norm": 16.550270080566406,
      "learning_rate": 1.753387533875339e-05,
      "loss": 0.9265,
      "step": 4550
    },
    {
      "epoch": 12.357723577235772,
      "grad_norm": 15.21157455444336,
      "learning_rate": 1.7528455284552848e-05,
      "loss": 1.2969,
      "step": 4560
    },
    {
      "epoch": 12.384823848238483,
      "grad_norm": 10.46440601348877,
      "learning_rate": 1.7523035230352305e-05,
      "loss": 1.1247,
      "step": 4570
    },
    {
      "epoch": 12.411924119241192,
      "grad_norm": 13.966622352600098,
      "learning_rate": 1.7517615176151764e-05,
      "loss": 0.9207,
      "step": 4580
    },
    {
      "epoch": 12.439024390243903,
      "grad_norm": 9.428882598876953,
      "learning_rate": 1.751219512195122e-05,
      "loss": 0.8639,
      "step": 4590
    },
    {
      "epoch": 12.466124661246612,
      "grad_norm": 10.625212669372559,
      "learning_rate": 1.7506775067750677e-05,
      "loss": 0.9472,
      "step": 4600
    },
    {
      "epoch": 12.493224932249323,
      "grad_norm": 10.613268852233887,
      "learning_rate": 1.7501355013550137e-05,
      "loss": 1.1545,
      "step": 4610
    },
    {
      "epoch": 12.520325203252032,
      "grad_norm": 6.7130818367004395,
      "learning_rate": 1.7495934959349593e-05,
      "loss": 1.0427,
      "step": 4620
    },
    {
      "epoch": 12.547425474254743,
      "grad_norm": 9.480582237243652,
      "learning_rate": 1.7490514905149053e-05,
      "loss": 0.8857,
      "step": 4630
    },
    {
      "epoch": 12.574525745257453,
      "grad_norm": 7.649380207061768,
      "learning_rate": 1.7485094850948513e-05,
      "loss": 0.9498,
      "step": 4640
    },
    {
      "epoch": 12.601626016260163,
      "grad_norm": 7.191319942474365,
      "learning_rate": 1.747967479674797e-05,
      "loss": 1.1694,
      "step": 4650
    },
    {
      "epoch": 12.628726287262873,
      "grad_norm": 15.735166549682617,
      "learning_rate": 1.747425474254743e-05,
      "loss": 1.1009,
      "step": 4660
    },
    {
      "epoch": 12.655826558265582,
      "grad_norm": 8.639644622802734,
      "learning_rate": 1.7468834688346885e-05,
      "loss": 0.974,
      "step": 4670
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 5.405998229980469,
      "learning_rate": 1.7463414634146342e-05,
      "loss": 1.0224,
      "step": 4680
    },
    {
      "epoch": 12.710027100271002,
      "grad_norm": 9.411730766296387,
      "learning_rate": 1.74579945799458e-05,
      "loss": 1.4726,
      "step": 4690
    },
    {
      "epoch": 12.737127371273713,
      "grad_norm": 13.421034812927246,
      "learning_rate": 1.7452574525745258e-05,
      "loss": 0.9256,
      "step": 4700
    },
    {
      "epoch": 12.764227642276422,
      "grad_norm": 10.264087677001953,
      "learning_rate": 1.7447154471544718e-05,
      "loss": 1.1452,
      "step": 4710
    },
    {
      "epoch": 12.791327913279133,
      "grad_norm": 9.523931503295898,
      "learning_rate": 1.7441734417344174e-05,
      "loss": 1.1305,
      "step": 4720
    },
    {
      "epoch": 12.818428184281842,
      "grad_norm": 10.498835563659668,
      "learning_rate": 1.7436314363143634e-05,
      "loss": 1.1742,
      "step": 4730
    },
    {
      "epoch": 12.845528455284553,
      "grad_norm": 15.685476303100586,
      "learning_rate": 1.743089430894309e-05,
      "loss": 1.0369,
      "step": 4740
    },
    {
      "epoch": 12.872628726287262,
      "grad_norm": 23.09590721130371,
      "learning_rate": 1.742547425474255e-05,
      "loss": 1.2146,
      "step": 4750
    },
    {
      "epoch": 12.899728997289973,
      "grad_norm": 7.566956996917725,
      "learning_rate": 1.7420054200542006e-05,
      "loss": 1.1253,
      "step": 4760
    },
    {
      "epoch": 12.926829268292684,
      "grad_norm": 11.108770370483398,
      "learning_rate": 1.7414634146341466e-05,
      "loss": 0.98,
      "step": 4770
    },
    {
      "epoch": 12.953929539295393,
      "grad_norm": 5.6278839111328125,
      "learning_rate": 1.7409214092140923e-05,
      "loss": 0.8265,
      "step": 4780
    },
    {
      "epoch": 12.981029810298104,
      "grad_norm": 15.031157493591309,
      "learning_rate": 1.740379403794038e-05,
      "loss": 1.1156,
      "step": 4790
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.537940379403794,
      "eval_f1": 0.5587266215383005,
      "eval_loss": 1.794060468673706,
      "eval_runtime": 2.7233,
      "eval_samples_per_second": 270.992,
      "eval_steps_per_second": 34.149,
      "step": 4797
    },
    {
      "epoch": 13.008130081300813,
      "grad_norm": 9.250422477722168,
      "learning_rate": 1.739837398373984e-05,
      "loss": 1.0181,
      "step": 4800
    },
    {
      "epoch": 13.035230352303524,
      "grad_norm": 14.180888175964355,
      "learning_rate": 1.7392953929539295e-05,
      "loss": 0.8595,
      "step": 4810
    },
    {
      "epoch": 13.062330623306233,
      "grad_norm": 9.798606872558594,
      "learning_rate": 1.7387533875338755e-05,
      "loss": 0.8778,
      "step": 4820
    },
    {
      "epoch": 13.089430894308943,
      "grad_norm": 5.547500133514404,
      "learning_rate": 1.7382113821138215e-05,
      "loss": 0.8715,
      "step": 4830
    },
    {
      "epoch": 13.116531165311653,
      "grad_norm": 3.455378532409668,
      "learning_rate": 1.737669376693767e-05,
      "loss": 1.1478,
      "step": 4840
    },
    {
      "epoch": 13.143631436314363,
      "grad_norm": 5.727319717407227,
      "learning_rate": 1.7371273712737127e-05,
      "loss": 0.9529,
      "step": 4850
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 9.086865425109863,
      "learning_rate": 1.7365853658536587e-05,
      "loss": 1.0766,
      "step": 4860
    },
    {
      "epoch": 13.197831978319783,
      "grad_norm": 9.38617992401123,
      "learning_rate": 1.7360433604336044e-05,
      "loss": 1.0847,
      "step": 4870
    },
    {
      "epoch": 13.224932249322492,
      "grad_norm": 18.485313415527344,
      "learning_rate": 1.7355013550135503e-05,
      "loss": 0.7556,
      "step": 4880
    },
    {
      "epoch": 13.252032520325203,
      "grad_norm": 11.736681938171387,
      "learning_rate": 1.734959349593496e-05,
      "loss": 0.7298,
      "step": 4890
    },
    {
      "epoch": 13.279132791327914,
      "grad_norm": 9.039433479309082,
      "learning_rate": 1.734417344173442e-05,
      "loss": 1.2528,
      "step": 4900
    },
    {
      "epoch": 13.306233062330623,
      "grad_norm": 13.323246002197266,
      "learning_rate": 1.733875338753388e-05,
      "loss": 1.1435,
      "step": 4910
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 9.38113021850586,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.8345,
      "step": 4920
    },
    {
      "epoch": 13.360433604336043,
      "grad_norm": 9.591266632080078,
      "learning_rate": 1.7327913279132792e-05,
      "loss": 0.9433,
      "step": 4930
    },
    {
      "epoch": 13.387533875338754,
      "grad_norm": 6.909919261932373,
      "learning_rate": 1.7322493224932252e-05,
      "loss": 0.9283,
      "step": 4940
    },
    {
      "epoch": 13.414634146341463,
      "grad_norm": 14.600615501403809,
      "learning_rate": 1.7317073170731708e-05,
      "loss": 1.0053,
      "step": 4950
    },
    {
      "epoch": 13.441734417344174,
      "grad_norm": 17.086517333984375,
      "learning_rate": 1.7311653116531168e-05,
      "loss": 1.0425,
      "step": 4960
    },
    {
      "epoch": 13.468834688346883,
      "grad_norm": 9.130837440490723,
      "learning_rate": 1.7306233062330624e-05,
      "loss": 1.0905,
      "step": 4970
    },
    {
      "epoch": 13.495934959349594,
      "grad_norm": 2.630732297897339,
      "learning_rate": 1.730081300813008e-05,
      "loss": 0.9015,
      "step": 4980
    },
    {
      "epoch": 13.523035230352303,
      "grad_norm": 20.231115341186523,
      "learning_rate": 1.729539295392954e-05,
      "loss": 1.1919,
      "step": 4990
    },
    {
      "epoch": 13.550135501355014,
      "grad_norm": 13.56441879272461,
      "learning_rate": 1.7289972899729e-05,
      "loss": 1.1444,
      "step": 5000
    },
    {
      "epoch": 13.577235772357724,
      "grad_norm": 7.080416202545166,
      "learning_rate": 1.7284552845528457e-05,
      "loss": 0.9972,
      "step": 5010
    },
    {
      "epoch": 13.604336043360433,
      "grad_norm": 4.1243720054626465,
      "learning_rate": 1.7279132791327916e-05,
      "loss": 1.1841,
      "step": 5020
    },
    {
      "epoch": 13.631436314363144,
      "grad_norm": 7.419930934906006,
      "learning_rate": 1.7273712737127373e-05,
      "loss": 1.02,
      "step": 5030
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 13.867769241333008,
      "learning_rate": 1.726829268292683e-05,
      "loss": 1.2592,
      "step": 5040
    },
    {
      "epoch": 13.685636856368564,
      "grad_norm": 14.619091987609863,
      "learning_rate": 1.726287262872629e-05,
      "loss": 0.6384,
      "step": 5050
    },
    {
      "epoch": 13.712737127371273,
      "grad_norm": 14.592812538146973,
      "learning_rate": 1.7257452574525745e-05,
      "loss": 1.076,
      "step": 5060
    },
    {
      "epoch": 13.739837398373984,
      "grad_norm": 26.33016586303711,
      "learning_rate": 1.7252032520325205e-05,
      "loss": 0.9836,
      "step": 5070
    },
    {
      "epoch": 13.766937669376693,
      "grad_norm": 11.775694847106934,
      "learning_rate": 1.7246612466124665e-05,
      "loss": 0.7595,
      "step": 5080
    },
    {
      "epoch": 13.794037940379404,
      "grad_norm": 12.953481674194336,
      "learning_rate": 1.724119241192412e-05,
      "loss": 1.4272,
      "step": 5090
    },
    {
      "epoch": 13.821138211382113,
      "grad_norm": 13.743839263916016,
      "learning_rate": 1.7235772357723578e-05,
      "loss": 0.9629,
      "step": 5100
    },
    {
      "epoch": 13.848238482384824,
      "grad_norm": 14.104079246520996,
      "learning_rate": 1.7230352303523037e-05,
      "loss": 0.9785,
      "step": 5110
    },
    {
      "epoch": 13.875338753387535,
      "grad_norm": 14.891520500183105,
      "learning_rate": 1.7224932249322494e-05,
      "loss": 1.3041,
      "step": 5120
    },
    {
      "epoch": 13.902439024390244,
      "grad_norm": 13.845355987548828,
      "learning_rate": 1.7219512195121954e-05,
      "loss": 1.166,
      "step": 5130
    },
    {
      "epoch": 13.929539295392955,
      "grad_norm": 7.0002360343933105,
      "learning_rate": 1.721409214092141e-05,
      "loss": 0.9736,
      "step": 5140
    },
    {
      "epoch": 13.956639566395664,
      "grad_norm": 13.606518745422363,
      "learning_rate": 1.7208672086720866e-05,
      "loss": 1.0795,
      "step": 5150
    },
    {
      "epoch": 13.983739837398375,
      "grad_norm": 11.405373573303223,
      "learning_rate": 1.7203252032520326e-05,
      "loss": 1.1488,
      "step": 5160
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.5365853658536586,
      "eval_f1": 0.5530279778236646,
      "eval_loss": 1.8091096878051758,
      "eval_runtime": 2.7126,
      "eval_samples_per_second": 272.061,
      "eval_steps_per_second": 34.284,
      "step": 5166
    },
    {
      "epoch": 14.010840108401084,
      "grad_norm": 4.977693557739258,
      "learning_rate": 1.7197831978319786e-05,
      "loss": 0.8352,
      "step": 5170
    },
    {
      "epoch": 14.037940379403794,
      "grad_norm": 12.679656982421875,
      "learning_rate": 1.7192411924119242e-05,
      "loss": 0.7728,
      "step": 5180
    },
    {
      "epoch": 14.065040650406504,
      "grad_norm": 10.872601509094238,
      "learning_rate": 1.7186991869918702e-05,
      "loss": 0.749,
      "step": 5190
    },
    {
      "epoch": 14.092140921409214,
      "grad_norm": 9.292691230773926,
      "learning_rate": 1.718157181571816e-05,
      "loss": 1.0067,
      "step": 5200
    },
    {
      "epoch": 14.119241192411923,
      "grad_norm": 14.236740112304688,
      "learning_rate": 1.7176151761517615e-05,
      "loss": 0.696,
      "step": 5210
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 10.186445236206055,
      "learning_rate": 1.7170731707317075e-05,
      "loss": 1.1171,
      "step": 5220
    },
    {
      "epoch": 14.173441734417343,
      "grad_norm": 18.502384185791016,
      "learning_rate": 1.716531165311653e-05,
      "loss": 1.023,
      "step": 5230
    },
    {
      "epoch": 14.200542005420054,
      "grad_norm": 15.215652465820312,
      "learning_rate": 1.715989159891599e-05,
      "loss": 0.8376,
      "step": 5240
    },
    {
      "epoch": 14.227642276422765,
      "grad_norm": 10.466506958007812,
      "learning_rate": 1.7154471544715447e-05,
      "loss": 0.971,
      "step": 5250
    },
    {
      "epoch": 14.254742547425474,
      "grad_norm": 17.61072540283203,
      "learning_rate": 1.7149051490514907e-05,
      "loss": 0.9672,
      "step": 5260
    },
    {
      "epoch": 14.281842818428185,
      "grad_norm": 5.451512336730957,
      "learning_rate": 1.7143631436314367e-05,
      "loss": 0.8021,
      "step": 5270
    },
    {
      "epoch": 14.308943089430894,
      "grad_norm": 18.056673049926758,
      "learning_rate": 1.7138211382113823e-05,
      "loss": 1.3365,
      "step": 5280
    },
    {
      "epoch": 14.336043360433605,
      "grad_norm": 8.187469482421875,
      "learning_rate": 1.713279132791328e-05,
      "loss": 1.09,
      "step": 5290
    },
    {
      "epoch": 14.363143631436314,
      "grad_norm": 9.108463287353516,
      "learning_rate": 1.712737127371274e-05,
      "loss": 0.862,
      "step": 5300
    },
    {
      "epoch": 14.390243902439025,
      "grad_norm": 4.444860458374023,
      "learning_rate": 1.7121951219512196e-05,
      "loss": 0.8176,
      "step": 5310
    },
    {
      "epoch": 14.417344173441734,
      "grad_norm": 9.640941619873047,
      "learning_rate": 1.7116531165311655e-05,
      "loss": 1.0515,
      "step": 5320
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 10.717005729675293,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 1.0198,
      "step": 5330
    },
    {
      "epoch": 14.471544715447154,
      "grad_norm": 16.36602783203125,
      "learning_rate": 1.7105691056910568e-05,
      "loss": 0.8729,
      "step": 5340
    },
    {
      "epoch": 14.498644986449865,
      "grad_norm": 20.975603103637695,
      "learning_rate": 1.7100271002710028e-05,
      "loss": 1.0822,
      "step": 5350
    },
    {
      "epoch": 14.525745257452574,
      "grad_norm": 32.446998596191406,
      "learning_rate": 1.7094850948509488e-05,
      "loss": 1.0757,
      "step": 5360
    },
    {
      "epoch": 14.552845528455284,
      "grad_norm": 13.829861640930176,
      "learning_rate": 1.7089430894308944e-05,
      "loss": 1.2995,
      "step": 5370
    },
    {
      "epoch": 14.579945799457995,
      "grad_norm": 7.631182670593262,
      "learning_rate": 1.7084010840108404e-05,
      "loss": 1.1647,
      "step": 5380
    },
    {
      "epoch": 14.607046070460704,
      "grad_norm": 13.648283958435059,
      "learning_rate": 1.707859078590786e-05,
      "loss": 1.1562,
      "step": 5390
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 6.106999397277832,
      "learning_rate": 1.7073170731707317e-05,
      "loss": 0.967,
      "step": 5400
    },
    {
      "epoch": 14.661246612466124,
      "grad_norm": 23.99195671081543,
      "learning_rate": 1.7067750677506776e-05,
      "loss": 0.863,
      "step": 5410
    },
    {
      "epoch": 14.688346883468835,
      "grad_norm": 10.933181762695312,
      "learning_rate": 1.7062330623306233e-05,
      "loss": 0.6753,
      "step": 5420
    },
    {
      "epoch": 14.715447154471544,
      "grad_norm": 11.947111129760742,
      "learning_rate": 1.7056910569105693e-05,
      "loss": 1.1182,
      "step": 5430
    },
    {
      "epoch": 14.742547425474255,
      "grad_norm": 7.43178129196167,
      "learning_rate": 1.7051490514905152e-05,
      "loss": 0.7864,
      "step": 5440
    },
    {
      "epoch": 14.769647696476964,
      "grad_norm": 7.301945686340332,
      "learning_rate": 1.704607046070461e-05,
      "loss": 0.9775,
      "step": 5450
    },
    {
      "epoch": 14.796747967479675,
      "grad_norm": 27.574411392211914,
      "learning_rate": 1.7040650406504065e-05,
      "loss": 1.042,
      "step": 5460
    },
    {
      "epoch": 14.823848238482384,
      "grad_norm": 10.480725288391113,
      "learning_rate": 1.7035230352303525e-05,
      "loss": 1.2809,
      "step": 5470
    },
    {
      "epoch": 14.850948509485095,
      "grad_norm": 15.435480117797852,
      "learning_rate": 1.702981029810298e-05,
      "loss": 1.246,
      "step": 5480
    },
    {
      "epoch": 14.878048780487806,
      "grad_norm": 11.940566062927246,
      "learning_rate": 1.702439024390244e-05,
      "loss": 0.9133,
      "step": 5490
    },
    {
      "epoch": 14.905149051490515,
      "grad_norm": 8.229594230651855,
      "learning_rate": 1.7018970189701897e-05,
      "loss": 1.0386,
      "step": 5500
    },
    {
      "epoch": 14.932249322493226,
      "grad_norm": 8.341578483581543,
      "learning_rate": 1.7013550135501354e-05,
      "loss": 0.6987,
      "step": 5510
    },
    {
      "epoch": 14.959349593495935,
      "grad_norm": 20.202926635742188,
      "learning_rate": 1.7008130081300817e-05,
      "loss": 1.0555,
      "step": 5520
    },
    {
      "epoch": 14.986449864498645,
      "grad_norm": 11.626514434814453,
      "learning_rate": 1.7002710027100273e-05,
      "loss": 1.1963,
      "step": 5530
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.5338753387533876,
      "eval_f1": 0.5515015193617698,
      "eval_loss": 1.8487240076065063,
      "eval_runtime": 2.7903,
      "eval_samples_per_second": 264.491,
      "eval_steps_per_second": 33.33,
      "step": 5535
    },
    {
      "epoch": 15.013550135501355,
      "grad_norm": 7.188787460327148,
      "learning_rate": 1.699728997289973e-05,
      "loss": 0.8013,
      "step": 5540
    },
    {
      "epoch": 15.040650406504065,
      "grad_norm": 12.380115509033203,
      "learning_rate": 1.699186991869919e-05,
      "loss": 1.0992,
      "step": 5550
    },
    {
      "epoch": 15.067750677506774,
      "grad_norm": 5.272646427154541,
      "learning_rate": 1.6986449864498646e-05,
      "loss": 0.9126,
      "step": 5560
    },
    {
      "epoch": 15.094850948509485,
      "grad_norm": 7.757810115814209,
      "learning_rate": 1.6981029810298106e-05,
      "loss": 0.8899,
      "step": 5570
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 10.979674339294434,
      "learning_rate": 1.6975609756097562e-05,
      "loss": 0.7126,
      "step": 5580
    },
    {
      "epoch": 15.149051490514905,
      "grad_norm": 13.100610733032227,
      "learning_rate": 1.697018970189702e-05,
      "loss": 0.8778,
      "step": 5590
    },
    {
      "epoch": 15.176151761517616,
      "grad_norm": 6.026295185089111,
      "learning_rate": 1.6964769647696478e-05,
      "loss": 1.1424,
      "step": 5600
    },
    {
      "epoch": 15.203252032520325,
      "grad_norm": 16.09685516357422,
      "learning_rate": 1.6959349593495938e-05,
      "loss": 0.9698,
      "step": 5610
    },
    {
      "epoch": 15.230352303523036,
      "grad_norm": 11.882762908935547,
      "learning_rate": 1.6953929539295394e-05,
      "loss": 0.9227,
      "step": 5620
    },
    {
      "epoch": 15.257452574525745,
      "grad_norm": 14.362146377563477,
      "learning_rate": 1.6948509485094854e-05,
      "loss": 0.713,
      "step": 5630
    },
    {
      "epoch": 15.284552845528456,
      "grad_norm": 11.207551002502441,
      "learning_rate": 1.694308943089431e-05,
      "loss": 0.9734,
      "step": 5640
    },
    {
      "epoch": 15.311653116531165,
      "grad_norm": 12.349505424499512,
      "learning_rate": 1.6937669376693767e-05,
      "loss": 0.8574,
      "step": 5650
    },
    {
      "epoch": 15.338753387533876,
      "grad_norm": 6.321864604949951,
      "learning_rate": 1.6932249322493227e-05,
      "loss": 0.8796,
      "step": 5660
    },
    {
      "epoch": 15.365853658536585,
      "grad_norm": 15.098175048828125,
      "learning_rate": 1.6926829268292683e-05,
      "loss": 0.7891,
      "step": 5670
    },
    {
      "epoch": 15.392953929539296,
      "grad_norm": 9.296311378479004,
      "learning_rate": 1.6921409214092143e-05,
      "loss": 0.9514,
      "step": 5680
    },
    {
      "epoch": 15.420054200542005,
      "grad_norm": 9.814912796020508,
      "learning_rate": 1.69159891598916e-05,
      "loss": 1.1696,
      "step": 5690
    },
    {
      "epoch": 15.447154471544716,
      "grad_norm": 15.061365127563477,
      "learning_rate": 1.691056910569106e-05,
      "loss": 1.0147,
      "step": 5700
    },
    {
      "epoch": 15.474254742547426,
      "grad_norm": 6.6434783935546875,
      "learning_rate": 1.6905149051490515e-05,
      "loss": 0.7814,
      "step": 5710
    },
    {
      "epoch": 15.501355013550135,
      "grad_norm": 15.658330917358398,
      "learning_rate": 1.6899728997289975e-05,
      "loss": 0.8043,
      "step": 5720
    },
    {
      "epoch": 15.528455284552846,
      "grad_norm": 9.560526847839355,
      "learning_rate": 1.689430894308943e-05,
      "loss": 0.9486,
      "step": 5730
    },
    {
      "epoch": 15.555555555555555,
      "grad_norm": 11.388895034790039,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.8801,
      "step": 5740
    },
    {
      "epoch": 15.582655826558266,
      "grad_norm": 23.302349090576172,
      "learning_rate": 1.6883468834688348e-05,
      "loss": 0.9789,
      "step": 5750
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 9.342683792114258,
      "learning_rate": 1.6878048780487804e-05,
      "loss": 1.0509,
      "step": 5760
    },
    {
      "epoch": 15.636856368563686,
      "grad_norm": 8.424849510192871,
      "learning_rate": 1.6872628726287264e-05,
      "loss": 0.795,
      "step": 5770
    },
    {
      "epoch": 15.663956639566395,
      "grad_norm": 2.978954315185547,
      "learning_rate": 1.686720867208672e-05,
      "loss": 0.6202,
      "step": 5780
    },
    {
      "epoch": 15.691056910569106,
      "grad_norm": 12.233671188354492,
      "learning_rate": 1.686178861788618e-05,
      "loss": 1.1759,
      "step": 5790
    },
    {
      "epoch": 15.718157181571815,
      "grad_norm": 15.021444320678711,
      "learning_rate": 1.685636856368564e-05,
      "loss": 1.0474,
      "step": 5800
    },
    {
      "epoch": 15.745257452574526,
      "grad_norm": 9.023761749267578,
      "learning_rate": 1.6850948509485096e-05,
      "loss": 1.0153,
      "step": 5810
    },
    {
      "epoch": 15.772357723577235,
      "grad_norm": 7.960277557373047,
      "learning_rate": 1.6845528455284556e-05,
      "loss": 1.0846,
      "step": 5820
    },
    {
      "epoch": 15.799457994579946,
      "grad_norm": 15.952203750610352,
      "learning_rate": 1.6840108401084012e-05,
      "loss": 0.9606,
      "step": 5830
    },
    {
      "epoch": 15.826558265582655,
      "grad_norm": 15.488496780395508,
      "learning_rate": 1.683468834688347e-05,
      "loss": 0.6156,
      "step": 5840
    },
    {
      "epoch": 15.853658536585366,
      "grad_norm": 12.365432739257812,
      "learning_rate": 1.682926829268293e-05,
      "loss": 0.79,
      "step": 5850
    },
    {
      "epoch": 15.880758807588077,
      "grad_norm": 5.3144707679748535,
      "learning_rate": 1.6823848238482385e-05,
      "loss": 0.9817,
      "step": 5860
    },
    {
      "epoch": 15.907859078590786,
      "grad_norm": 9.759753227233887,
      "learning_rate": 1.6818428184281845e-05,
      "loss": 1.2826,
      "step": 5870
    },
    {
      "epoch": 15.934959349593496,
      "grad_norm": 8.566437721252441,
      "learning_rate": 1.6813008130081304e-05,
      "loss": 1.006,
      "step": 5880
    },
    {
      "epoch": 15.962059620596206,
      "grad_norm": 25.256162643432617,
      "learning_rate": 1.680758807588076e-05,
      "loss": 1.1464,
      "step": 5890
    },
    {
      "epoch": 15.989159891598916,
      "grad_norm": 20.545936584472656,
      "learning_rate": 1.6802168021680217e-05,
      "loss": 0.9316,
      "step": 5900
    },
    {
      "epoch": 16.0,
      "eval_accuracy": 0.532520325203252,
      "eval_f1": 0.5520117782668432,
      "eval_loss": 1.8276628255844116,
      "eval_runtime": 2.6551,
      "eval_samples_per_second": 277.961,
      "eval_steps_per_second": 35.028,
      "step": 5904
    }
  ],
  "logging_steps": 10,
  "max_steps": 36900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "total_flos": 2668978693525248.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
